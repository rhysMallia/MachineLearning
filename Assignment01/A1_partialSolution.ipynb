{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'>  </font></div>\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> Assignment 01: **Partial Solution**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is not a complete solution for assignment 01.** The perpose of this notebook is to provide feedback on assignment 1 and it is not intended to be a compete solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./Assignment1/train_data.csv')\n",
    "data_test = pd.read_csv('./Assignment1/test_data.csv')\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratary Data Analysis (EDA)\n",
    "\n",
    "First lets see what variable types are in our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several categotical variables in the dataset. Lets assign the correct data type to them. Variables `CCS Procedure Code` and `APR Severity of Illness Code` are read as integer values even though they should be categories. Lets fix those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(data_train.columns):\n",
    "    if data_train[col].dtypes != np.int64:\n",
    "        data_train[col] = data_train[col].astype('category')\n",
    "        data_test[col] = data_test[col].astype('category')\n",
    "\n",
    "data_train['APRSeverityOfIllnessCode'] = data_train['APRSeverityOfIllnessCode'].astype('category')\n",
    "data_train['CCSProcedureCode'] = data_train['CCSProcedureCode'].astype('category')\n",
    "\n",
    "data_test['APRSeverityOfIllnessCode'] = data_test['APRSeverityOfIllnessCode'].astype('category')\n",
    "data_test['CCSProcedureCode'] = data_test['CCSProcedureCode'].astype('category')\n",
    "data_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are no missing values in the train/test datasets.**\n",
    "\n",
    "\n",
    "Lets see how the test and train data are distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i, col in enumerate(data_train.columns):\n",
    "\n",
    "    if col != 'LengthOfStay' and  col != 'ID':\n",
    "        plt.subplot(4,5,i+1)\n",
    "        if data_train[col].dtypes != np.int64:\n",
    "            plt.bar(data_train[col].value_counts().index.astype(str), \n",
    "                    data_train[col].value_counts().values/np.sum(data_train[col].value_counts().values), \n",
    "                    alpha=0.3, color='r')\n",
    "            plt.bar(data_test[col].value_counts().index.astype(str), \n",
    "                    data_test[col].value_counts().values/np.sum(data_test[col].value_counts().values), \n",
    "                    alpha=0.3, color='b')\n",
    "        else:\n",
    "            plt.hist(data_train[col], alpha=0.3, color='r', density=True)\n",
    "            plt.hist(data_test[col], alpha=0.3, color='b', density=True)\n",
    "\n",
    "        plt.title(col)\n",
    "        plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_train['LengthOfStay'], alpha=0.3, color='r', density=True)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "1. Some new `Health Service Area` elements present in the training set. Therefore this column cannot be used in the prediction model. **This information is important when deciding on the test/train split**. \n",
    "2. Test set and train set are fairly equally distributed (except for points 1). \n",
    "3. Output variable `Length of Stay` column is Highly Skewed. Most data has `Length of Stay` around 2 - Classes will be inbalanced.\n",
    "4. Sever categorical attributes have high imbalance: `Type of Admission`, `APR Severity of Illness Code`, `Race`, `Emergency Department Indicator`, `Payment Typology 1`\n",
    "5. `Birth Weight` might have some outliers. Further exploration needed.\n",
    "6. The value range for numerical attributes vary significantly. Should consider normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets investigate column `Birth Weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(data_train['BirthWeight'])\n",
    "plt.title('BirthWeight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of rows with birth weight > 5000: \", np.sum(data_train['BirthWeight']>5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train[data_train['BirthWeight']<5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Several outliers seen in the birthweight column. Can remove the rows that has birthweight larger than 5000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now explore the relationship between attributes and the output variable. For this analysis we can use a subset of the data that is balaced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_sample = data_train.groupby('LengthOfStay')\n",
    "data_train_sample = data_train_sample.apply(lambda x: x.sample(data_train_sample.size().min()*10, replace=True).reset_index(drop=True))\n",
    "\n",
    "data_train_sample.reset_index(inplace = True, drop=True)\n",
    "data_train_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_sample['LengthOfStay'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the relationship between numerical variables and `Length of Stay`. Scatterplots can be used here, but they are not informative for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,5))\n",
    "i=1\n",
    "for col in data_train_sample.columns:\n",
    "    if col != 'ID':\n",
    "        if data_train_sample[col].dtypes == np.int64 and col != 'LengthOfStay':\n",
    "            plt.subplot(1,10,i)\n",
    "            sns.boxplot(x='LengthOfStay',y=col,data=data_train_sample)\n",
    "            i = i+1\n",
    "            plt.title(col)\n",
    "\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** \n",
    "1. The median of the birthweight distribution linearly decrease with the length of stay.\n",
    "2. Some facilities may be specialized in short stay procedures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the relationship between categorical variables and Length of Stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,5))\n",
    "i=1\n",
    "for col in data_train_sample.columns:\n",
    "  \n",
    "  if data_train_sample[col].dtypes != np.int64 and col != 'LengthOfStay':\n",
    "    plt.subplot(1,8,i)\n",
    "    ax = sns.boxplot(y='LengthOfStay',x=col,data=data_train_sample)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "    i = i+1\n",
    "    plt.title(col)\n",
    "\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "1. `Type of Admission`,`ARP Severity of Illness Code`,`Emergency Department Indicator` seems to be highly correlated with `Length of Stay`.\n",
    "2.  Medicare patients seems to be always short stay. \n",
    "3. `CCS Procedure Code` also seem to be informative in predicting `Length of Stay`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation analysis between numberical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "corr = data_train_sample.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. `Length of Stay` show a strong negative corealtion with `Birth Weight` and some positive correlation with `charge in facility`.\n",
    "2. Strong correlation between `Average cost in county`, `Average charges in county`, `Average cost in facility`, etc... - **Need regularization to overcome these**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Convert categorical variable to one-hot. \n",
    "\n",
    "*Should avoid using get_dummies as we have already split test and train.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def oneHotEncodeColumn(df, colName, drop_col, enc_onehot=None):\n",
    "    if enc_onehot is None:\n",
    "        enc_onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc_onehot.fit(df[[colName]])\n",
    "\n",
    "    onehot_ = enc_onehot.transform(df[[colName]]).toarray()\n",
    "\n",
    "    for i in range(len(enc_onehot.categories_[0])):\n",
    "        if enc_onehot.categories_[0][i] != drop_col:\n",
    "            if type(enc_onehot.categories_[0][i]) == str:\n",
    "                df[colName + '_' + enc_onehot.categories_[0][i]] = onehot_[:,i]\n",
    "            else:\n",
    "                df[colName + '_' + str(enc_onehot.categories_[0][i])] = onehot_[:,i]\n",
    "\n",
    "    df = df.drop([colName], axis=1)\n",
    "\n",
    "    return df, enc_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, gender_enc = oneHotEncodeColumn(data_train, 'Gender', 'U')\n",
    "data_train, race_enc = oneHotEncodeColumn(data_train, 'Race', 'Other Race')\n",
    "data_train, ToA_enc = oneHotEncodeColumn(data_train, 'TypeOfAdmission', 'Newborn')\n",
    "data_train, CCSPC_enc = oneHotEncodeColumn(data_train, 'CCSProcedureCode', -1)\n",
    "data_train, paymentTop_enc = oneHotEncodeColumn(data_train, 'PaymentTypology', 'Miscellaneous/Other')\n",
    "data_train, Egcy_enc = oneHotEncodeColumn(data_train, 'EmergencyDepartmentIndicator', 'N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`APR Severity of Illness Code` is categorical, however the order of its value is important therefore it should be left as numerical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['APRSeverityOfIllnessCode'] = data_train['APRSeverityOfIllnessCode'].astype(np.int64)/4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data\n",
    "\n",
    "Functions to do min-max and robust scaling. You can also do a power transform of some of the attributes, However I have not demostrated that here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def minMaxScale(df, colName, min_max_scaler=None):\n",
    "    if min_max_scaler is None: \n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        min_max_scaler.fit(df[[colName]])\n",
    "\n",
    "    df[colName] = min_max_scaler.transform(df[[colName]])\n",
    "    return df, min_max_scaler\n",
    "\n",
    "\n",
    "def robustScale(df, colName, min_max_scaler=None):\n",
    "    if min_max_scaler is None: \n",
    "        min_max_scaler = preprocessing.RobustScaler()\n",
    "        min_max_scaler.fit(df[[colName]])\n",
    "\n",
    "    df[colName] = min_max_scaler.transform(df[[colName]])\n",
    "    return df, min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "corr = data_train.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to decide the performance measure: For this We will use **macro f1_score** as:\n",
    " - The target class in the data set is heavyly biased to one class.\n",
    " - We would like to see similer performance in both classes. (Not high performace in one class and poor performance in other class)\n",
    " \n",
    "Give the complexity of the problem we would like to achive around .65 f1_score. **This needs to be justified**\n",
    "\n",
    "\n",
    "Next lets split the data. Here I am splitting the data randomly. However a better approch would be use the `HealthServiceArea` to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data_trainSplit, data_valSplit = train_test_split(data_train, test_size=0.2)\n",
    "# data_trainSplit, data_testSplit = train_test_split(data_trainSplit, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trainSplit = data_train[(data_train['HealthServiceArea'] == 'New York City') \n",
    "                             | (data_train['HealthServiceArea'] == 'Southern Tier') ]\n",
    "data_valSplit = data_train[(data_train['HealthServiceArea'] == 'Long Island') \n",
    "                           | (data_train['HealthServiceArea'] == 'Central NY') ]\n",
    "data_testSplit = data_train[(data_train['HealthServiceArea'] == 'Capital/Adirond') \n",
    "                            | (data_train['HealthServiceArea'] == 'Western NY') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minmax scale all numerical attributes except `Birth Weight`. `Birth Weight` had outliers we can use robust scaler (more for demostration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2scale = ['AverageCostInCounty','AverageChargesInCounty',\n",
    "              'AverageCostInFacility','AverageChargesInFacility',\n",
    "              'AverageIncomeInZipCode', 'BirthWeight']\n",
    "\n",
    "minmax_scaler_hold = dict()\n",
    "\n",
    "for col in cols2scale:\n",
    "    data_trainSplit, minmax_scaler_hold[col] =  minMaxScale(data_trainSplit, col)\n",
    "    data_valSplit, _ =  minMaxScale(data_valSplit, col, minmax_scaler_hold[col])\n",
    "    data_testSplit, _ =  minMaxScale(data_testSplit, col, minmax_scaler_hold[col])\n",
    "\n",
    "    \n",
    "# data_train, minmax_scaler_hold['BirthWeight'] =  robustScale(data_train, 'BirthWeight')\n",
    "# data_valSplit, _ =  robustScale(data_valSplit, 'BirthWeight', minmax_scaler_hold['BirthWeight'])\n",
    "# data_testSplit, _ =  robustScale(data_testSplit, 'BirthWeight', minmax_scaler_hold['BirthWeight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = data_valSplit.drop(['LengthOfStay', 'HealthServiceArea', 'ID'], axis=1) #remove data leakage features\n",
    "y_val = (data_valSplit[['LengthOfStay']]>3).astype(np.int64)\n",
    "\n",
    "X_train = data_trainSplit.drop(['LengthOfStay', 'HealthServiceArea', 'ID'], axis=1) #remove data leakage features\n",
    "y_train = (data_trainSplit[['LengthOfStay']]>3).astype(np.int64)\n",
    "\n",
    "X_test = data_testSplit.drop(['LengthOfStay', 'HealthServiceArea', 'ID'], axis=1) #remove data leakage features\n",
    "y_test = (data_testSplit[['LengthOfStay']]>3).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data size: \", y_train.shape[0])\n",
    "print(\"Val data size: \", y_val.shape[0])\n",
    "print(\"Test data size: \", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y_train['LengthOfStay'].value_counts().index.astype(str), \n",
    "         y_train['LengthOfStay'].value_counts().values, alpha=0.3, color='r')\n",
    "plt.xlabel('Number of data Instances')\n",
    "plt.ylabel('Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Class 0 has significatly less data instances than class 1. May need balancing.\n",
    "\n",
    "Lets compute a class balance weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), np.ravel(y_train.to_numpy()))\n",
    "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "plt.barh([str(i) for i in class_weights], [class_weights[i] for i in class_weights], alpha=0.3, color='r')\n",
    "plt.xlabel('Class Weight')\n",
    "plt.ylabel('Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for fitting model and drawing feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_sample_weights(y, class_weights):\n",
    "    sample_weight = np.ravel(y)*class_weights[1] + (1 - np.ravel(y))*class_weights[0]\n",
    "    return sample_weight\n",
    "\n",
    "\n",
    "def fit_classification_model(clf, xtrain, ytrain, xval, yval, class_weights={0: 1, 1: 1}, sample_weights=None, print_report=False):\n",
    "    clf.fit(xtrain, np.ravel(ytrain), sample_weight=sample_weights)\n",
    "    ytrain_pred = clf.predict(xtrain)\n",
    "    yval_pred = clf.predict(xval)\n",
    "    \n",
    "    f1_val = f1_score(yval, yval_pred, average='macro')\n",
    "    f1_train = f1_score(ytrain, ytrain_pred, average='macro')\n",
    "\n",
    "    # rec_val = recall_score(yval, yval_pred, average='macro')\n",
    "    # rec_train = recall_score(ytrain, ytrain_pred, average='macro')\n",
    "    \n",
    "    bce_val = log_loss(yval, yval_pred, sample_weight=get_sample_weights(yval, class_weights))\n",
    "    bce_train = log_loss(ytrain, ytrain_pred, sample_weight=get_sample_weights(ytrain, class_weights))\n",
    "    \n",
    "    if print_report:\n",
    "        print(classification_report(yval, yval_pred))\n",
    "        \n",
    "    return clf, f1_train, f1_val, bce_train, bce_val\n",
    "\n",
    "\n",
    "\n",
    "def test_classification_model(clf, xval, yval, print_report=False):\n",
    "    yval_pred = clf.predict(xval)\n",
    "    \n",
    "    rec_val = recall_score(yval, yval_pred, average='macro')\n",
    "    \n",
    "    if print_report:\n",
    "        print(classification_report(yval, yval_pred))\n",
    "        print(rec_val)\n",
    "        \n",
    "    return clf, rec_val, bce_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model 1\n",
    "\n",
    "### Linear classifier\n",
    "First lets start with a simple linear classifier. This will set the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "\n",
    "clf, rec_train, rec_val, bce_train, bce_val = fit_classification_model(clf, X_train, y_train, X_val, y_val, class_weights=class_weights, print_report=True)\n",
    "\n",
    "print(f'Train set F1-Score: {rec_train:.4f} BCE: {bce_train:.4f}')\n",
    "print(f'Valid set F1-Score: {rec_val:.4f} BCE: {bce_val:.4f}')\n",
    "\n",
    "Results['baseline'] = {'clf': clf, 'rec_train': rec_train, 'rec_val': rec_val, 'bce_train':bce_train, 'bce_val': bce_val}\n",
    "\n",
    "plot_confusion_matrix(clf, X_val, y_val, normalize='true')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** confusion matrix shows that most data instances are classified as 1\n",
    "\n",
    "We noticed that there is a class imbalance in the dataset. Therefore we need to take action to correct it. one way of doing this is to add a balacing weight. Lets try that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight='balanced',max_iter=500)\n",
    "\n",
    "clf, rec_train, rec_val, bce_train, bce_val = fit_classification_model(clf, X_train, y_train, X_val, y_val, class_weights=class_weights, print_report=True)\n",
    "\n",
    "print(f'Train set F1-Score: {rec_train:.4f} BCE: {bce_train:.4f}')\n",
    "print(f'Valid set F1-Score: {rec_val:.4f} BCE: {bce_val:.4f}')\n",
    "\n",
    "Results['balanced_linear'] = {'clf': clf, 'rec_train': rec_train, 'rec_val': rec_val, 'bce_train':bce_train, 'bce_val': bce_val}\n",
    "\n",
    "plot_confusion_matrix(clf, X_val, y_val, normalize='true')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The performance improves when we add the balancing term. Now there are more correct classfications in class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not observe overfitting when we fit the logistic regression model. However, We observed multi-coleniarity in the attributesin our EDA. Therefore regularization is a good idea. Lets try this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression has one hyper parameter.  Lambda. Lets tune it using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2,2,num=10)\n",
    "\n",
    "val_performace = list()\n",
    "train_performace = list()\n",
    "for C in Cs:\n",
    "    clf = LogisticRegression(max_iter=500, solver='liblinear', class_weight='balanced', penalty='l1', C=C)\n",
    "    clf, rec_train, rec_val, bce_train, bce_val = fit_classification_model(clf, X_train, y_train, X_val, y_val, class_weights=class_weights)\n",
    "    \n",
    "    val_performace.append(bce_val)\n",
    "    train_performace.append(bce_train)\n",
    "    print(f'Done .. C = {C:.5f} Train F1 = {rec_train:.3f}, Val F1 = {rec_val:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Cs, train_performace, 'ro')\n",
    "plt.plot(Cs, val_performace, 'b.')\n",
    "plt.xscale('log')\n",
    "plt.legend(['Train Performance','Val Performance'])\n",
    "plt.xlabel('Model Capacity')\n",
    "plt.ylabel('Weighted Binary Cross-Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Very slight overfitting, both Train and val performance are similar for every C value in range 1-10000. Lets select C=0.2 as it seems to be the best value - lowest gap and lowest val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=500, solver='liblinear', class_weight='balanced', penalty='l1', C=0.07743)\n",
    "\n",
    "clf, rec_train, rec_val, bce_train, bce_val = fit_classification_model(clf, X_train, y_train, X_val, y_val, class_weights=class_weights)\n",
    "\n",
    "Results['balanced_linear_lasso'] = {'clf': clf, 'rec_train': rec_train, 'rec_val': rec_val, 'bce_train':bce_train, 'bce_val': bce_val}\n",
    "\n",
    "print(f'Train set F1-Score: {rec_train:.4f} BCE: {bce_train:.4f}')\n",
    "print(f'Valid set F1-Score: {rec_val:.4f} BCE: {bce_val:.4f}')\n",
    "\n",
    "plot_confusion_matrix(clf, X_val, y_val, normalize='true')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** The performance is similar to the `balanced_linear` classifier.However we may prefer the Lasso model as it gives a simpler model. More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Classification\n",
    "\n",
    "The above observation seem to indicate that the model capacity maybe low. We can increase the capacity by going to polynormial features.\n",
    "This is also supported by the EDA as we so some non linier relationships between output varialble and some attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly2 = PolynomialFeatures(2)\n",
    "poly2.fit(X_train)\n",
    "X_train_poly2 = poly2.transform(X_train)\n",
    "X_val_poly2 = poly2.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit a balanced regularized model to the polynormial features. Hyper parameter in Lasso is tuned with hold-out validation. **Can also tune the poly degree parameter**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2,2,num=10)\n",
    "\n",
    "val_performace = list()\n",
    "train_performace = list()\n",
    "for C in Cs:\n",
    "    clf = LogisticRegression(max_iter=500, solver='liblinear', class_weight='balanced', penalty='l1', C=C)\n",
    "    clf, rec_train, rec_val, bce_train, bce_val = fit_classification_model(clf, X_train_poly2, y_train, X_val_poly2, y_val, class_weights=class_weights)\n",
    "    \n",
    "    val_performace.append(bce_val)\n",
    "    train_performace.append(bce_train)\n",
    "    print(f'Done .. C = {C:.5f} Train F1 = {rec_train:.3f}, Val F1 = {rec_val:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Cs, train_performace, 'ro')\n",
    "plt.plot(Cs, val_performace, 'b.')\n",
    "plt.xscale('log')\n",
    "plt.legend(['Train Performance','Val Performance'])\n",
    "plt.xlabel('Model Capacity')\n",
    "plt.ylabel('Weighted Binary Cross-Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Very slight overfitting, both Train and val performance are similar for every C value in range 1-10000. Lets select C=0.02 as it seems to be the best value - lowest gap and lowest val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=500, solver='liblinear', class_weight='balanced', penalty='l1', C=0.07743)\n",
    "clf, rec_train, rec_val, bce_train, bce_val = fit_classification_model(clf, X_train_poly2, y_train, X_val_poly2, y_val, class_weights=class_weights)\n",
    "\n",
    "Results['balanced_poly2_lasso'] = {'clf': clf, 'rec_train': rec_train, 'rec_val': rec_val, 'bce_train':bce_train, 'bce_val': bce_val}\n",
    "\n",
    "print(f'Train set F1-Score: {rec_train:.4f} BCE: {bce_train:.4f}')\n",
    "print(f'Valid set F1-Score: {rec_val:.4f} BCE: {bce_val:.4f}')\n",
    "\n",
    "plot_confusion_matrix(clf, X_val_poly2, y_val, normalize='true')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using polynormial features it introduces a second hyper parameter. The polynormial degree. Lets tune that hyper-parameter. Unfortunately most conputers allow us only to go upto degree 3, as they will run out of memory. therefore we will do this next instead of goiing throuhg a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "poly.fit(X_train)\n",
    "X_train_poly3 = poly.transform(X_train)\n",
    "X_val_poly3 = poly.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2,1,num=10)\n",
    "\n",
    "val_performace = list()\n",
    "train_performace = list()\n",
    "for C in Cs:\n",
    "    clf = LogisticRegression(max_iter=500, solver='liblinear', class_weight='balanced', penalty='l1', C=C)\n",
    "    clf, rec_train, rec_val, bce_train, bce_val = fit_classification_model(clf, X_train_poly3, y_train, X_val_poly3, y_val, class_weights=class_weights)\n",
    "    \n",
    "    val_performace.append(bce_val)\n",
    "    train_performace.append(bce_train)\n",
    "    print(f'Done .. C = {C:.5f} Train Recall = {rec_train:.3f}, Val Recall = {rec_val:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Cs, train_performace, 'ro')\n",
    "plt.plot(Cs, val_performace, 'b.')\n",
    "plt.xscale('log')\n",
    "plt.legend(['Train Performance','Val Performance'])\n",
    "plt.xlabel('Model Capacity')\n",
    "plt.ylabel('Weighted Binary Cross-Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overffiring seen. The best C value is around 0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=500, solver='liblinear', class_weight='balanced', penalty='l1', C=0.04642)\n",
    "clf, rec_train, rec_val, bce_train, bce_val = fit_classification_model(clf, X_train_poly3, y_train, X_val_poly3, y_val, class_weights=class_weights)\n",
    "\n",
    "Results['balanced_poly3_lasso'] = {'clf': clf, 'rec_train': rec_train, 'rec_val': rec_val, 'bce_train':bce_train, 'bce_val': bce_val}\n",
    "\n",
    "print(f'Train set Recall-Score: {rec_train:.4f} BCE: {bce_train:.4f}')\n",
    "print(f'Valid set Recall-Score: {rec_val:.4f} BCE: {bce_val:.4f}')\n",
    "\n",
    "plot_confusion_matrix(clf, X_val_poly3, y_val, normalize='true')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model 2\n",
    "Lets try an ADABoost classifier. This is not an eleigible classifier and it is okay to not have tested this. The following code can easyly be converted to a randomforest that is eleigible.\n",
    "\n",
    "Need to tune hyperparameters. I have not done this proparly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=200)\n",
    "clf, rec_train, rec_val, bce_train, bce_val = fit_classification_model(clf, X_train, y_train, X_val, y_val, class_weights=class_weights, sample_weights=get_sample_weights(y_train, class_weights))\n",
    "\n",
    "Results['adaBoost'] = {'clf': clf, 'rec_train': rec_train, 'rec_val': rec_val, 'bce_train':bce_train, 'bce_val': bce_val}\n",
    "\n",
    "print(f'Train set F1-Score: {rec_train:.4f} BCE: {bce_train:.4f}')\n",
    "print(f'Valid set F1-Score: {rec_val:.4f} BCE: {bce_val:.4f}')\n",
    "\n",
    "plot_confusion_matrix(clf, X_val, y_val, normalize='true')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison and ultimate judgment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets compare the results for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PrettyTable\n",
    "from prettytable import PrettyTable\n",
    "t = PrettyTable(['Model', 'Train F1', 'Train BCE', 'Val F1', 'Val BCE'])\n",
    "\n",
    "for i in Results:\n",
    "    rec_train = Results[i]['rec_train']\n",
    "    rec_val = Results[i]['rec_val']\n",
    "    bce_train = Results[i]['bce_train']\n",
    "    bce_val = Results[i]['bce_val']\n",
    "    t.add_row([i, f'{rec_train:.3f}',f'{bce_train:.3f}', f'{rec_val:.3f}', f'{bce_val:.3f}'])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_lin = ['baseline','balanced_linear','balanced_linear_lasso']\n",
    "clfs_poly2 = ['balanced_poly2_lasso']\n",
    "clfs_poly3 = ['balanced_poly3_lasso']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15,10))\n",
    "ax = axes.flatten()\n",
    "for i, c in enumerate(clfs_lin):\n",
    "    plot_confusion_matrix(Results[c]['clf'], X_val, y_val, normalize='true', ax=ax[i], cmap='Blues')  \n",
    "    ax[i].title.set_text(c)\n",
    "\n",
    "for i, c in enumerate(clfs_poly2):\n",
    "    plot_confusion_matrix(Results[c]['clf'], X_val_poly2, y_val, normalize='true', ax=ax[i+len(clfs_lin)], cmap='Blues')  \n",
    "    ax[i+len(clfs_lin)].title.set_text(c)\n",
    "\n",
    "for i, c in enumerate(clfs_poly3):\n",
    "    plot_confusion_matrix(Results[c]['clf'], X_val_poly3, y_val, normalize='true', ax=ax[i+len(clfs_lin)+1], cmap='Blues')  \n",
    "    ax[i+len(clfs_lin)+1].title.set_text(c)\n",
    "    \n",
    "for i, c in enumerate(['adaBoost', ]):\n",
    "    plot_confusion_matrix(Results[c]['clf'], X_val, y_val, normalize='true', ax=ax[i+len(clfs_lin)+2], cmap='Blues')  \n",
    "    ax[i+len(clfs_lin)+2].title.set_text(c)   \n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the eligible classifiers the baseline is clearly suboptimal. Howerver the other four have similar Recall-Score with `balanced_poly3_lasso` having the best. Since the Recall-scores are close we need to investigate the models to make a judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `balanced_linear` and `balanced_linear_lasso` have similar performance which would be better?\n",
    "\n",
    "To decide lets look at the feature importance for the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def plot_permutation_importance(clf, X_val, y_val):\n",
    "    r = permutation_importance(clf, X_val, y_val,n_repeats=30)\n",
    "    inx = np.argsort(r.importances_mean)\n",
    "    \n",
    "    plt.barh(X_val.columns[inx], r.importances_mean[inx])\n",
    "    plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plot_permutation_importance(Results['balanced_linear']['clf'], X_val, y_val)\n",
    "plt.subplot(1,2,2)\n",
    "plot_permutation_importance(Results['balanced_linear_lasso']['clf'], X_val, y_val)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still both look very similar. However the regularized classifier totally ignores some feature e.g. `Gender_?`, `Average ? in County`. These were known to be colinier with other feature in the EDA. Therefore out of the two we can select the `balanced_linear_lasso` - given everything is equal we select the least complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets pick one out of the nonlinier classifiers. Again we can look at the performance as well as other aspects of the classifier like feature importnace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature_names = ['x'.join(['{}^{}'.format(pair[0],pair[1]) for pair in tuple if pair[1]!=0]) for tuple in [zip(X_train.columns,p) for p in poly2.powers_]]\n",
    "X_val_poly2_df = pd.DataFrame(X_val_poly2, columns = target_feature_names)\n",
    "\n",
    "plt.figure(figsize=(20,100))\n",
    "plt.subplot(1,2,1)\n",
    "plot_permutation_importance(Results['balanced_poly2_lasso']['clf'], X_val_poly2_df, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the two non linier classifiers I would select `balanced_poly2_lasso` as it is simpler and has similer performace to `balanced_poly3_lasso`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimate Judgment\n",
    "\n",
    "Give the two models we have narrowed down eirlier, **I would finally select  `balanced_poly2_lasso`**: this is because it has higher performance and agree with some observations we made during EDA: some attributes had non linier relationship with the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the final model\n",
    "\n",
    "Here we ask questions about the model we selected and see if it agrees with our view of the problem and our values.\n",
    "\n",
    "## Biases\n",
    "How does the model perfrom for different health service areas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_poly2 = poly2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, f1_test, _ = test_classification_model(Results['balanced_poly2_lasso']['clf'], X_test_poly2, y_test)\n",
    "print('Test set F1 Score: ', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Results['balanced_poly2_lasso']['clf'],\n",
    "                      X_test_poly2, y_test, normalize='true', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rechold = list()\n",
    "for hsa in data_testSplit['HealthServiceArea'].unique():\n",
    "    X_test_hsa = X_test_poly2[data_testSplit['HealthServiceArea'] == hsa]\n",
    "    y_test_hsa = y_test[data_testSplit['HealthServiceArea'] == hsa]\n",
    "    _, rec_test, _ = test_classification_model(Results['balanced_poly2_lasso']['clf'], X_test_hsa, y_test_hsa)\n",
    "    rechold.append(rec_test)\n",
    "\n",
    "plt.barh(data_valSplit['HealthServiceArea'].unique(), rechold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the models perform for different genderes? is there a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = ['Gender_F', 'Gender_M']\n",
    "\n",
    "rechold = list()\n",
    "for hsa in gender:\n",
    "    X_test_hsa = X_test_poly2[data_testSplit[hsa] == 1]\n",
    "    y_test_hsa = y_test[data_testSplit[hsa] == 1]\n",
    "    _, rec_test, _ = test_classification_model(Results['balanced_poly2_lasso']['clf'], X_test_hsa, y_test_hsa)\n",
    "    rechold.append(rec_test)\n",
    "\n",
    "plt.barh(gender, rechold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the models perform for different races? is there a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = ['Race_Black/African American', 'Race_Multi-racial', 'Race_White']\n",
    "\n",
    "rechold = list()\n",
    "for hsa in race:\n",
    "    X_test_hsa = X_test_poly2[data_testSplit[hsa] == 1]\n",
    "    y_test_hsa = y_test[data_testSplit[hsa] == 1]\n",
    "    _, rec_test, _ = test_classification_model(Results['balanced_poly2_lasso']['clf'], X_test_hsa, y_test_hsa)\n",
    "    rechold.append(rec_test)\n",
    "\n",
    "plt.barh(race, rechold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `test_data.csv` Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('./Assignment1/test_data.csv')\n",
    "\n",
    "for i, col in enumerate(data_test.columns):\n",
    "    if data_test[col].dtypes != np.int64:\n",
    "        data_test[col] = data_test[col].astype('category')\n",
    "\n",
    "data_test['APRSeverityOfIllnessCode'] = data_test['APRSeverityOfIllnessCode'].astype('category')\n",
    "data_test['CCSProcedureCode'] = data_test['CCSProcedureCode'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test, _ = oneHotEncodeColumn(data_test, 'Gender', 'U', gender_enc)\n",
    "data_test, _ = oneHotEncodeColumn(data_test, 'Race', 'Other Race', race_enc)\n",
    "data_test, _ = oneHotEncodeColumn(data_test, 'TypeOfAdmission', 'Newborn', ToA_enc)\n",
    "data_test, _ = oneHotEncodeColumn(data_test, 'CCSProcedureCode', -1, CCSPC_enc)\n",
    "data_test, _ = oneHotEncodeColumn(data_test, 'PaymentTypology', 'Miscellaneous/Other', paymentTop_enc)\n",
    "data_test, _ = oneHotEncodeColumn(data_test, 'EmergencyDepartmentIndicator', 'N', Egcy_enc)\n",
    "\n",
    "data_test['APRSeverityOfIllnessCode'] = data_test['APRSeverityOfIllnessCode'].astype(np.int64)/4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2scale = ['AverageCostInCounty','AverageChargesInCounty',\n",
    "              'AverageCostInFacility','AverageChargesInFacility',\n",
    "              'AverageIncomeInZipCode', 'BirthWeight']\n",
    "\n",
    "for col in cols2scale:\n",
    "    data_test, _ =  minMaxScale(data_test, col, minmax_scaler_hold[col])\n",
    "\n",
    "# data_test, _ =  robustScale(data_test, 'BirthWeight', minmax_scaler_hold['BirthWeight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_test.drop(['HealthServiceArea', 'ID'], axis=1) #remove data leakage features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_poly2 = poly2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Results['balanced_poly2_lasso']['clf']\n",
    "data_test['LengthOfStay'] =clf.predict(X_test_poly2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[['ID','LengthOfStay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv('Ruwan_s123456_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
