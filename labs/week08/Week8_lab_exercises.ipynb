{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'>  </font></div>\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> Week 8 Lab Exercises: **Reinforcement learning**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab you will be:\n",
    "\n",
    "1. Learning how to use OpenAI gym.  \n",
    "2. Implement Q-learning to solve a well-known toy reinforcement learning problem called [Cartpole problem](https://gym.openai.com/envs/CartPole-v1/).  \n",
    "\n",
    "\n",
    "**It is tricky to get OpenAI gym to work on the AWS (without using the AWS specific functionality). Therefore please run this lab on the anaconda environment on your PC.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole with RL\n",
    "`Cartpole` is a classic control Reinforcement Learning problem that was first introduced by by Barto, Sutton, and Anderson [Barto83]. \n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole Problem definition:\n",
    ">Objective: Prevent the pole (pendulum) from falling over.\n",
    "\n",
    ">State: {Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity}\n",
    "\n",
    ">Action: {Push cart to the left, Push cart to the right}.\n",
    "\n",
    ">Reward: +1 for every timestep that the pole remains upright (we will change this slightly in our implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym \n",
    "OpenAI Gym is a Python package comprising a selection of RL environments, ranging from simple “toy” environments to more challenging environments, including simulated robotics environments and Atari video game environments.\n",
    "It was developed with the aim of becoming a standardized environment and benchmark for RL research.\n",
    "In this Lab, we will use the OpenAI Gym Cartpole environment to demonstrate how to get started in using this exciting tool and show how Q-learning can be used to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "Lets first import the libraries required for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "!pip install gym\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only uncomment the following block if the visualization of the environment gives an error**. On mac you need to install pyglet version 1.5.11 to get the gym environment to render. The installation will give an error, but it will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyglet==1.5.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with this environment, import and initialize it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `env.reset()` command resets the environemnt and return the initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore the state space and the action space og the Cartpole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('State space: ', env.observation_space)\n",
    "print('Action space: ', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the state space is a 4-dimensional space, so each state observation is a vector of 4 (float) values, and that the action space comprises two discrete actions (Push cart to the left, Push cart to the right). By default, the two actions are represented by the integers 0 and 1. How about the state space? What are the limis of the state space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('State space Low: ', env.observation_space.low)\n",
    "print('State space High: ', env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the first state variable (Cart Position) has a range [-4.8, 4.8] and the second state variable (Cart Velocity) has a range [-$\\infty$, $\\infty$].... The state space of the environment is a continuous state space, which means that there are infinitely many state-action pairs, making it impossible to build a Q table. As a solution to this problem we can descritize the state space. One simple discritization is to conver the stat espace to a grid where there are 20 grid positions in along each dimention. Note that we have truncated the two dimetions with infinite limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numBins = 20\n",
    "bins = [np.linspace(-4.8, 4.8, numBins),\n",
    "        np.linspace(-4, 4, numBins),\n",
    "        np.linspace(-.418, .418, numBins),\n",
    "        np.linspace(-4, 4, numBins)]\n",
    "obsSpaceSize = len(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write a function that will convert a continuous state vector to a descrete one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state, bins, obsSpaceSize):\n",
    "    stateIndex = []\n",
    "    for i in range(obsSpaceSize):\n",
    "        stateIndex.append(np.digitize(state[i], bins[i]) - 1) # -1 will turn bin into index\n",
    "    return tuple(stateIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now make some random actions in the environment and see what the output will be. For this we need a function to plot the output of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "step_index = 0\n",
    "while done != True:\n",
    "    action = env.action_space.sample()    # get a random action from the set of actions\n",
    "    state, reward, done, info = env.step(action) # perform the action and receive new state and reward\n",
    "    d_state = discretize_state(state, bins, obsSpaceSize)\n",
    "    show_state(env, step=step_index, info='State ({},{},{},{}) Reward: {}'.format(d_state[0], d_state[1],d_state[2], d_state[3], reward))\n",
    "    step_index = step_index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the pole remain upright for a long time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning \n",
    "\n",
    "We are going to use Q-learning for this task. Lets first define some hyper parameters. You may change them to get better performance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 50000\n",
    "\n",
    "# parameters for epsilon decay policy\n",
    "EPSILON = 1 # not a constant, going to be decayed\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES // 2\n",
    "epsilon_decay_value = EPSILON / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "#for testing\n",
    "N_TEST_RUNS = 100\n",
    "TEST_INTERVAL = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to test a given model. The function outputs two performace values. Was the run successful (pole upright for 200 steps), and the number of steps it ran. \n",
    "\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Identify if there are better performace measures to be used for this task and discuss with tutor. </font>**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(Qtable):\n",
    "    state = env.reset()\n",
    "    dstate = discretize_state(state, bins, obsSpaceSize)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action = np.argmax(Qtable[dstate]) \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        dstate = discretize_state(state, bins, obsSpaceSize)\n",
    "        steps = steps + 1\n",
    "        \n",
    "    success_run = 0\n",
    "    if steps > 199:\n",
    "        success_run = 1\n",
    "        \n",
    "    return success_run, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets develop a function for Q learning. The function prototype is given below. Assume that Q is a numpy matrix with dimentions (number of elemets for Cart Position, number of elements for Cart Velocity, number of elements for Pole Angle, number of elements for Pole Angular Velocity, number of actions).\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> Complete the following function using the knowladge gained in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, QTable):\n",
    "    # Env: The OpenAI gym environment\n",
    "    # QTable: Initial Q table\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        done = False\n",
    "        \n",
    "        # get the initial state\n",
    "        state = env.reset()\n",
    "        discretState = discretize_state(state, bins, obsSpaceSize)\n",
    "        \n",
    "        epsilon = EPSILON\n",
    "    \n",
    "        steps = 0;\n",
    "        while done != True:   \n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy for explore vs exploitation\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                # select the best action according to Qtable (exploitation)\n",
    "                # TODO\n",
    "            else:\n",
    "                # select a random action (exploration)\n",
    "                # TODO\n",
    "                \n",
    "            # Step and Get the next state and reward\n",
    "            # TODO\n",
    "            \n",
    "            \n",
    "            #Allow for terminal states\n",
    "            if done and steps < 200:\n",
    "                reward = -375    # what is happending here?\n",
    "                \n",
    "            # Update the Q table\n",
    "            # TODO\n",
    "                                     \n",
    "            # Update variables\n",
    "            discretState = discretStateNew\n",
    "            steps = steps + 1\n",
    "            \n",
    "            \n",
    "        # Update epsilon\n",
    "        if END_EPSILON_DECAYING >= episode and episode >= START_EPSILON_DECAYING:\n",
    "            epsilon -= epsilon_decay_value\n",
    "        \n",
    "        # test the model and print test results\n",
    "        if episode % TEST_INTERVAL == 0:\n",
    "            success_run_ = list()\n",
    "            steps_ = list()\n",
    "            for i in range(N_TEST_RUNS):\n",
    "                success_run, steps = test_model(QTable)\n",
    "                success_run_.append(success_run)\n",
    "                steps_.append(steps)\n",
    "                \n",
    "            print('Testing at Episode {}:'.format(episode))\n",
    "            print('\\t Successful Runs: {}/{}'.format(np.sum(success_run_), N_TEST_RUNS) )\n",
    "            print('\\t Average Steps: {}'.format(np.mean(steps_)))\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return QTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Solutions\n",
    "\n",
    "If you are struggling with the above function, a sample solution has been provided.\n",
    "Only use this if you have **made your absolute best attempts** at implementing the function yourself.\n",
    "The purpose of this lab is to understand common aspects of RL algorithm, though the Q-learning algorithm.\n",
    "You will gain significantly less out of this lab if you don't try to solve the problems yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Identify what is happenning in the epsilon decay policy. Discuss with tutor. </font>**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q table randomly\n",
    "Initial_QTable = np.random.uniform(low=-2, high=0, size=([numBins] * obsSpaceSize + [env.action_space.n]))\n",
    "\n",
    "# Run Q-learning algorithm\n",
    "QTable = QLearning(env, Initial_QTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how the we can perform the task with the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "dState = discretize_state(state, bins, obsSpaceSize)\n",
    "done = False\n",
    "step_index = 0\n",
    "while done != True:\n",
    "    action = np.argmax(QTable[dState]) \n",
    "    state, reward, done, info = env.step(action)\n",
    "    dState = discretize_state(state, bins, obsSpaceSize)\n",
    "    show_state(env, step=step_index, info='State ({},{},{},{}) Reward: {}'.format(dState[0], dState[1],dState[2], dState[3], reward))\n",
    "    step_index = step_index + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
