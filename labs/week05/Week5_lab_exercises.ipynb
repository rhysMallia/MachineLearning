{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> Week 5 Lab Exercises: **Training a Classification Model & Typical ML process**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "During the last couple of weeks we learned about how to read data, do exploratory data analysis (EDA) and prepare data for training and training a ML model. However, we did not specifically discuss the typical ML pipeline. In this lab we will go through a typical ML model development process using a classification task as an example.\n",
    "\n",
    "The lab assumes that you have completed the labs for week 2-4. If you havent yet, please do so before attempting this lab. \n",
    "\n",
    "The lab can be executed on either your own machine (with anaconda installation) or on AWS educate classroom setup for the course. \n",
    "- Please refer canvas for instructions on installing anaconda python or setting up AWS Sagemaker notebook: [Introduction to Amazon Web Services (AWS) Classrooms](https://rmit.instructure.com/courses/79534/pages/introduction-to-amazon-web-services-aws-classrooms?module_item_id=2952364)\n",
    "\n",
    "\n",
    "## Objective\n",
    "- Continue to familiarise with Python and other ML packages\n",
    "- Learn to train a model for classification problem\n",
    "- Practice typical ML model development process. \n",
    "\n",
    "\n",
    "## Dataset\n",
    "In this lab, we will be using the `Cardiotocography Data Set` from UCI [Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Cardiotocography). The dataset consists of measurements of fetal heart rate (FHR) and uterine contraction (UC) features on cardiotocograms classified by expert obstetricians. 2126 fetal cardiotocograms (CTGs) were automatically processed and the respective diagnostic features measured. The CTGs were also classified by three expert obstetricians and a consensus classification label assigned to each of them. Classification was respect the fetal state (0=normal; 1=suspect; 2=pathologic). The version used in this lab is a modified version of the original dataset and, the columns of the original dataset are:\n",
    "\n",
    "1. LB - FHR baseline (beats per minute)  \n",
    "2. AC - # of accelerations per second  \n",
    "3. FM - # of fetal movements per second  \n",
    "4. UC - # of uterine contractions per second  \n",
    "5. DL - # of light decelerations per second  \n",
    "6. DS - # of severe decelerations per second  \n",
    "7. DP - # of prolongued decelerations per second  \n",
    "8. ASTV - percentage of time with abnormal short term variability\n",
    "9. MSTV - mean value of short term variability\n",
    "10. ALTV - percentage of time with abnormal long term variability\n",
    "11. MLTV - mean value of long term variability\n",
    "12. Width - width of FHR histogram\n",
    "13. Min - minimum of FHR histogram\n",
    "14. Max - Maximum of FHR histogram\n",
    "15. Nmax - # of histogram peaks\n",
    "16. Nzeros - # of histogram zeros\n",
    "17. Mode - histogram mode\n",
    "18. Mean - histogram mean\n",
    "19. Median - histogram median\n",
    "20. Variance - histogram variance\n",
    "21. Tendency - histogram tendency\n",
    "22. TARGET: NSP - fetal state class code (0=normal; 1=suspect; 2=pathologic)\n",
    "\n",
    "**The task for this lab is to predict if a new fetal measurement is 0=normal; 1=suspect; 2=pathologic.**\n",
    "\n",
    "\n",
    "First, ensure the data file is located within the Jupyter workspace. \n",
    "- If you are on the local machine copy the file (`Cardiotocography_Data_Set_subset.csv`) to your current folder.\n",
    "- If you are on AWS you can upload the data to the notebook instance by clicking the `upload files` icon on the left sidebar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset and some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./Cardiotocography_Data_Set_subset.csv', delimiter=',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the target to match the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set((data['NSP']).astype(np.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['NSP'] =  (data['NSP']).astype(np.int) - 1 \n",
    "print(set((data['NSP']).astype(np.int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we need to check if there are missing values and identify an action to handle them. Lets check if the data has any missing values. You can use the pandas `describe` to see if there are any columns with less number of items than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What observations did you make?\n",
    "\n",
    "> <span style=\"font-size:1em;\">✔</span> **Observations:** \n",
    "> - We can see that the `mode` column has only 2113 items while other columns all have 2126 items.\n",
    "\n",
    "\n",
    "if there are missing values in the dataset, they are generally represented as Nan Values. Lets check for Nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(data).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Mode` column has 13 NaN values. We can find which instances/rows this corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[pd.isna(data).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What are the possible actions we can take?\n",
    "\n",
    "> <span style=\"font-size:1em;\">✔</span> **Actions:** \n",
    "> - We can remove the above rows from the dataset. This will lead to loss of some information as we will lose the other attribute information in those rows.\n",
    "> - We can replace the missing values with zero (or the mean of that column with missing values). Need to see if this is reasonable for a given attribute. \n",
    "> - We can use another feature(s) to predict the missing values and use that. \n",
    "\n",
    "**For this problem** we can observe that the `Mode` and the `Median` (or `Mean`) has a very strong correlation (See EDA results that appear later). therefore we can use the value of the `Median` to replace the missing values of `Mode`.\n",
    "Generally we might have to train a ML model to predict the missing attributes (x: `median` , y: `Mode`). However for this problem we can even directly replace the missing mode values without building a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[pd.isna(data['Mode']), 'Mode'] = data.loc[pd.isna(data['Mode']), 'Median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(data).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "In the following, I have shown you several techniques that can be used to analyse the data for this dataset. However, this is not an exhaustive set of techniques. \n",
    "\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: We have leaned about doing an EDA in lab 02 and you are left to explore the possible techniques tor this problem. Do not limit your self to techniques presented in the class. </font>**  \n",
    "\n",
    "Lets first see if there are patterns in scatter plots of two variable at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "g = sns.PairGrid(data, vars=['LB','FM' , 'ASTV' , 'ALTV', 'Width', \n",
    "                             'Nmax', 'Mode', 'Mean', 'Median', 'Variance'], hue=\"NSP\")\n",
    "g.map(sns.scatterplot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What observations did you make?\n",
    "\n",
    "> <span style=\"font-size:1em;\">✔</span> **Observations:** \n",
    "> - Some plots show that a non-linear decision boundary might be able to separate the two classes. e.g. ASTV vs ASTL\n",
    "> - Some plots show that a linear decision boundary might be able to separate the two classes. e.g. Median vs Variance\n",
    "\n",
    "lets also observe the correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "corr = data.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What observations did you make?\n",
    "\n",
    "Since we have discussed this in class, I will leave this as an exercise for you. Discuss with the lab demonstrator in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that is interesting is to see the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['NSP'].hist(figsize=(5,5))\n",
    "plt.xlabel('NSP')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What observations did you make?\n",
    "\n",
    "Since we have discussed this in class, I will leave this as an exercise for you. Discuss with the lab demonstrator in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Model development process\n",
    "\n",
    "As discussed in the lecture, the typical ML model development process consists of 4 steps, lets go through each and see how it is done. \n",
    "\n",
    "1. **Determine your goals**: Performance metric and target value. Problem dependent.\n",
    "\n",
    "2. **Setup the experiment**: Setup the test/validation data, visualisers and debuggers needed to determine bottlenecks in performance (overfitting/under-fitting, feature importance).\n",
    "\n",
    "3. **Default Baseline Model**: Identify the components of end-to-end pipeline including - Baseline Models, cost functions, optimisation.\n",
    "\n",
    "4. **Make incremental changes**: Repeatedly make incremental changes such as gathering new data, adjusting hyper-parameters, or changing algorithms, based on specific findings from your instrumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the performance (evaluation) metric\n",
    "There are many performance metrics that apply to this problem such as `accuracy_score`, `f1_score`, etc. More information on performance metrics available in sklearn can be found at: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "The insights gained in the EDA becomes vital in determining the performance metric. Try to identify the characteristics that are important in making this decision from the EDA results. Use your judgment to pick the best performance measure - discuss with the lab demonstrator to see if the performance measure you came up with is appropriate. \n",
    "\n",
    "\n",
    "In this task, I want to give equal importance to all three classes therefore I will select `macro-averaged` `f1_score` as my performance measure and I wish to achieve a target value of 85% f1_score. \n",
    "\n",
    "F1-score is NOT the only performance measure that can be used for this problem.\n",
    "\n",
    "\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Read this article on [f1_score](https://sebastianraschka.com/faq/docs/computing-the-f1-score.html).</font>**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the experiment - data splits\n",
    "\n",
    "Next **what data should we use to evaluate the performance?**\n",
    "\n",
    "\n",
    "We can generate \"simulated\" unseen data in several methods\n",
    "1. Hold-Out validation\n",
    "2. Cross-Validation\n",
    "\n",
    "Usually you will select a technique that is most appropriate to the dataset given to you. However as we are interested in learning about the techniques Lets look at both techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out Validation\n",
    "\n",
    "In hold out validation we divide the data into 3 subsets:\n",
    "1. Training: to obtaining the parameters or the weights of the hypothesis\n",
    "2. Validation: for tuning hyper-parameters and model selection.\n",
    "3. To evaluate the performance of the developed model. DO NOT use this split to set or tune any element of the model.\n",
    "\n",
    "For this example lets divide the data into 60/20/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_, test_data = train_test_split(data, test_size=0.2, \n",
    "                                              shuffle=True,random_state=0)\n",
    "    \n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data, val_data = train_test_split(train_data_, test_size=0.25, \n",
    "                                            shuffle=True,random_state=0)\n",
    "    \n",
    "print(train_data.shape[0], val_data.shape[0], test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert the data to np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data.drop(['NSP',], axis=1).to_numpy()\n",
    "train_y = train_data[['NSP']].to_numpy()\n",
    "\n",
    "test_X = test_data.drop(['NSP',], axis=1).to_numpy()\n",
    "test_y = test_data[['NSP']].to_numpy()\n",
    "\n",
    "val_X = val_data.drop(['NSP',], axis=1).to_numpy()\n",
    "val_y = val_data[['NSP']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets setup some functions to get the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_f1_scores(clf, train_X, train_y, val_X, val_y):\n",
    "    train_pred = clf.predict(train_X)\n",
    "    val_pred = clf.predict(val_X)\n",
    "    \n",
    "    train_f1 = f1_score(train_y, train_pred, average='macro')\n",
    "    val_f1 = f1_score(val_y, val_pred, average='macro')\n",
    "    \n",
    "    return train_f1, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "We need to select a baseline mode to do this task. I am going to select `regularised polynomial logistic regression` for this example.\n",
    "\n",
    "*There are better models than this, however we only know logistic regression technique that can be used for this problem at the moment, therefore out choices are limited and the decision is simple.* If we had other options, we need to use our knowledge on those techniques and  the EDA to select the best base model. \n",
    "\n",
    "The polynomial model is justified because in the EDA we can see that a non-linear decision boundary can separate the classes. regularisation is justified because we have correlated attributes and in EDA we also had some features where a linear decision boundary looked appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "poly.fit(train_X)\n",
    "train_X = poly.transform(train_X)\n",
    "test_X = poly.transform(test_X)\n",
    "val_X = poly.transform(val_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using polynomial features it is very important to scale the features. Lets do a minmax normalisation. Again you can leverage the EDA to select the appropriate scaling mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_X)\n",
    "\n",
    "train_X = scaler.transform(train_X)\n",
    "val_X = scaler.transform(val_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the un-regularised linear model - just to check if everything is in order. You will notice a warning saying the max_iter was reached. \n",
    "\n",
    "Ideally we would increase the number of maximum iterations and see if it solves the problem. For now lets ignore the warning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, penalty='none', solver='saga', \n",
    "                         max_iter=1000, \n",
    "                         class_weight='balanced').fit(train_X, train_y.ravel())\n",
    "\n",
    "train_f1, val_f1 = get_f1_scores(clf, train_X, train_y, val_X, val_y)\n",
    "print(\"Train F1-Score score: {:.3f}\".format(train_f1))\n",
    "print(\"Validation F1-Score score: {:.3f}\".format(val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task the baseline model achieved good training performance. However we can see a gap between the Train Accuracy and the Validation Accuracy (generalisation GAP). \n",
    "\n",
    "**What can we do when there is a GAP between Train and Validation performance?**\n",
    "\n",
    "- We can apply regularisation. The process is important. we start with a base model and then improve it based on our observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply regularisation\n",
    "\n",
    "When applying regularisation we need to select the lambda value. For this we can use\n",
    "1. Grid search\n",
    "2. Random search\n",
    "\n",
    "\n",
    "We will do grid search in this example. in grid search we establish a set of lambda values in a frid. Selecting the range of lambda values is a process mostly done with trial and error.\n",
    "\n",
    "ones we select a set of lambda values, we train a classifier for each of those lambda values and evaluate the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_paras = np.logspace(-5, 1, num=25)    # establish the lambda values to test (grid)\n",
    "\n",
    "# Then search\n",
    "train_performace = list()\n",
    "valid_performace = list()\n",
    "\n",
    "for lambda_para in lambda_paras:\n",
    "    clf = LogisticRegression(penalty='l2', C = 1.0/lambda_para, \n",
    "                             random_state=0, solver='liblinear', max_iter=1000 , \n",
    "                             class_weight='balanced').fit(train_X, train_y.ravel())\n",
    "    \n",
    "    train_f1, val_f1 = get_f1_scores(clf, train_X, train_y, val_X, val_y)\n",
    "    \n",
    "    train_performace.append(train_f1)\n",
    "    valid_performace.append(val_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the training and validation performance for each lambda value in out lambda values set and see what is the best lambda value. You might have to repeat the process of selecting lambda values if the results are not as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1.0/lambda_para for lambda_para in lambda_paras], \n",
    "         [tp for tp in train_performace], 'r-')\n",
    "plt.plot([1.0/lambda_para for lambda_para in lambda_paras], \n",
    "         [vp for vp in valid_performace], 'b--')\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xlabel('Model Capacity')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What lambda value would you pick as your final value?\n",
    "\n",
    "We generally pick the lambda value that corresponds to the maximum validation performance and minimum generalisation GAP. In this case it is ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', C = 40.0, random_state=0, \n",
    "                         solver='liblinear', max_iter=1000, \n",
    "                         class_weight='balanced').fit(train_X, train_y.ravel())\n",
    "\n",
    "train_f1, val_f1 = get_f1_scores(clf, train_X, train_y, val_X, val_y)\n",
    "print(\"Train F1-Score score: {:.3f}\".format(train_f1))\n",
    "print(\"Validation F1-Score score: {:.3f}\".format(val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Is this a better model than the un-regularised model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Have we achieved our Target?\n",
    "\n",
    "- We set out target at 85% fi-score. However we have only reached approximately 83% here. What would you do next? Discuss with your tutor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Identify other hyper-parameters and device a mechanism to tune then under hold-out validation framework.</font>**  \n",
    "\n",
    "Discuss your answer with the lab demonstrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the hypothesis (or model)\n",
    "\n",
    "Lets now assume that we have reached the best performance we can achieve. The next step is to test the hypothesis (or model) we have developed and see if we can trust the model to generalise to unseen data. This is where the test data comes in. \n",
    "\n",
    "Lets see how the model performs on test data. Below I have shown some useful techniques that can be used to observe the testing performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_pred = clf.predict(test_X)\n",
    "    \n",
    "print(classification_report(test_y, test_pred,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "disp = plot_confusion_matrix(clf, test_X, test_y,\n",
    "                                 cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What is your conclusion? Are you confident about the model?\n",
    "\n",
    "We should also use model visualisation techniques discussed in last weeks lab to get a better understanding of the model. Since the techniques were introduced last week, it will be an exercise for you.\n",
    "\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Use appropriate visualisation techniques to understand the model you have developed.</font>**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "To understand cross validation lets also use that technique on the same problem.\n",
    "\n",
    "Again it is good practice to retain a test set to get performance on the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, \n",
    "                                             shuffle=True,random_state=0)\n",
    "    \n",
    "print(train_data.shape[0], test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next couple of steps are the same as we did in hold out validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data.drop(['NSP',], axis=1).to_numpy()\n",
    "train_y = train_data[['NSP']].to_numpy()\n",
    "\n",
    "test_X = test_data.drop(['NSP',], axis=1).to_numpy()\n",
    "test_y = test_data[['NSP']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "poly.fit(train_X)\n",
    "train_X = poly.transform(train_X)\n",
    "test_X = poly.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_X)\n",
    "\n",
    "train_X = scaler.transform(train_X)\n",
    "val_X = scaler.transform(val_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now apply cross validation. Here I am applying 5 fold cross validation. I have reduce the max_iter to 100 in order to complete reduce the computation time for the lab. This is not a good practise, you can increase it as appropriate if you have time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "lambda_paras = np.logspace(-10, 2, num=5)\n",
    "\n",
    "cv_results = dict()\n",
    "\n",
    "for lambda_para in lambda_paras:\n",
    "    clf = LogisticRegression(penalty='l2', C = 1.0/lambda_para, \n",
    "                             solver='liblinear', max_iter=100, \n",
    "                             class_weight='balanced')\n",
    "    \n",
    "    scores = cross_validate(clf, train_X, train_y.ravel(), \n",
    "                            scoring=f1_scorer, return_estimator=True,\n",
    "                            return_train_score=True, cv=5)\n",
    "    \n",
    "    cv_results[lambda_para] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "val_means = [np.mean(cv_results[lambda_para]['test_score']) \n",
    "             for lambda_para in lambda_paras]\n",
    "\n",
    "val_std = [np.std(cv_results[lambda_para]['test_score']) \n",
    "           for lambda_para in lambda_paras]\n",
    "\n",
    "train_means = [np.mean(cv_results[lambda_para]['train_score']) \n",
    "               for lambda_para in lambda_paras]\n",
    "\n",
    "train_std = [np.std(cv_results[lambda_para]['train_score']) \n",
    "             for lambda_para in lambda_paras]\n",
    "\n",
    "ax.errorbar([1.0/lambda_para for lambda_para in lambda_paras], \n",
    "            val_means,\n",
    "            yerr=val_std)\n",
    "\n",
    "ax.errorbar([1.0/lambda_para for lambda_para in lambda_paras], \n",
    "            train_means,\n",
    "            yerr=train_std)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel('Classification Error')\n",
    "plt.xlabel('Model Capacity')\n",
    "plt.legend(['Validation','Training',])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What observations did you make?\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> What will be the best lambda value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Identify other hyper-parameters and device a mechanism to tune then under hold-out validation framework.</font>**  \n",
    "\n",
    "Discuss your answer with the lab demonstrator.\n",
    "\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Test the final model and use appropriate visualisation techniques to understand the model you have developed.</font>**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Full Cardiotocography Data Set\n",
    "\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Use the full Cardiotocography Data Set in `Cardiotocography_Data_Set.csv` and see if you can develop a better model.</font>**  \n",
    "> Now you see how to do this task with the smaller dataset. Repeat the same process for the complete dataset.\n",
    "\n",
    "\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: There are more convenient functions in sklearn to do grid search for hyper parameter tuning. These are specially useful when there are many hyper parameters to tune. GridSearchCV function is one example. Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) and understand how it works. </font>** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
