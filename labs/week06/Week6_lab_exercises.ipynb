{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> Week 6 Lab Exercises: **Decision Trees**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "During the last couple of weeks we learned about the typical ML model development process. In this weeks lab we will explore decision tree based models. \n",
    "\n",
    "The lab assumes that you have completed the labs for week 2-5. If you havent yet, please do so before attempting this lab. \n",
    "\n",
    "The lab can be executed on either your own machine (with anaconda installation) or on AWS educate classroom setup for the course. \n",
    "- Please refer canvas for instructions on installing anaconda python or setting up AWS Sagemaker notebook: [Introduction to Amazon Web Services (AWS) Classrooms](https://rmit.instructure.com/courses/79534/pages/introduction-to-amazon-web-services-aws-classrooms?module_item_id=2952364)\n",
    "\n",
    "\n",
    "## Objective\n",
    "- Continue to familiarise with Python and other ML packages.\n",
    "- Learning classification decision trees from both categorical and continuous numerical data\n",
    "- Comparing the performance of various trees after pruned.\n",
    "- Learning regression decision trees and comparing these models to regression models from previous labs.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed. \n",
    "\n",
    "#### Input variables:\n",
    "- Bank client data:\n",
    "    1. age (numeric)\n",
    "    2. job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\", \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \n",
    "    3. marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\n",
    "    4. education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n",
    "    5. default: has credit in default? (binary: \"yes\",\"no\")\n",
    "    6. balance: average yearly balance, in euros (numeric) \n",
    "    7. housing: has housing loan? (binary: \"yes\",\"no\")\n",
    "    8. loan: has personal loan? (binary: \"yes\",\"no\")\n",
    "- Related with the last contact of the current campaign:\n",
    "    9. contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") \n",
    "    10. day: last contact day of the month (numeric)\n",
    "    11. month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
    "    12. duration: last contact duration, in seconds (numeric)\n",
    "- Other attributes:\n",
    "    13. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "    14. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n",
    "    15. previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "    16. poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n",
    "    \n",
    "#### Output variable (desired target):   \n",
    "    17. y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n",
    "    \n",
    "This dataset is public available for research. The details are described in Moro et al., 2011.\n",
    "\n",
    "Moro et al., 2011: S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \n",
    "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011. \n",
    "\n",
    "Lets read the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./bank-full.csv', delimiter=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains categorical and numerical attributes. Lets convert the categorical columns to categorical data type in pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].dtype == object:\n",
    "        data[col] = data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn’s classification decision tree learner doesn’t work with categorical attributes.  It only works with continuous numeric attributes.  The target class, however, must be categorical.  So the categorical attributed must be converted into a suitable continuous format. Helpfully, Pandas can do this. \n",
    "\n",
    "First, split the data into the target class and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataY = data['y']\n",
    "dataX = data.drop(columns='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use Pandas to generate \"numerical\" versions of the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataXExpand = pd.get_dummies(dataX)\n",
    "dataXExpand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the categories are expanded into boolean (yes/no, that is, 1/0) values that can be treated as continuous numerical values. It’s not ideal, but it will allow a correct decision tree to be learned.\n",
    "\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> Why is it necessary to convert the attributes into boolean representations, rather than just convert them into integer values? What problem would be caused by converting the attributes into integers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target class also needs to be pre-processed.  The target will be treated by sklearn as a category, but sklearn requires that these categories are represented as integers (not strings). To convert the strings into numbers, the preprocessing. LabelEncoder class from sklearn can be used, as shown below. The two print statements show how to convert in both directions (strings to integers, and vice-versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(dataY)\n",
    "class_labels = le.inverse_transform([0,1])\n",
    "dataY = le.transform(dataY)\n",
    "print(dataY)\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Since we have covered how to do EDA in the previous labs, this section is left as an exercise for you. Complete the EDA and use the information to justify the decisions made in the subsequent code blocks.</font>**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the performance (evaluation) metric\n",
    "There are many performance metrics that apply to this problem such as `accuracy_score`, `f1_score`, etc. More information on performance metrics available in sklearn can be found at: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "The insights gained in the EDA becomes vital in determining the performance metric. Try to identify the characteristics that are important in making this decision from the EDA results. Use your judgment to pick the best performance measure - discuss with the lab demonstrator to see if the performance measure you came up with is appropriate. \n",
    "\n",
    "\n",
    "In this task, I want to give equal importance to all classes. Therefore I will select `macro-averaged` `f1_score` as my performance measure and I wish to achieve a target value of 75% f1_score. \n",
    "\n",
    "F1-score is NOT the only performance measure that can be used for this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the experiment - data splits\n",
    "\n",
    "Next **what data should we use to evaluate the performance?**\n",
    "\n",
    "\n",
    "We can generate \"simulated\" unseen data in several methods\n",
    "1. Hold-Out validation\n",
    "2. Cross-Validation\n",
    "\n",
    "Lets use hold out validation for this experiment.\n",
    "\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Use the knowledge from last couple of weeks to split the data appropriately.</font>**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_X_, test_data_X, train_data_y_ , test_data_y = train_test_split(dataXExpand, dataY, test_size=0.2, \n",
    "                                              shuffle=True,random_state=0)\n",
    "    \n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_X, val_data_X, train_data_y, val_data_y = train_test_split(train_data_X_, train_data_y_, test_size=0.25, \n",
    "                                            shuffle=True,random_state=0)\n",
    "    \n",
    "print(train_data_X.shape, val_data_X.shape, test_data_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data_X.to_numpy()\n",
    "train_y = train_data_y\n",
    "\n",
    "test_X = test_data_X.to_numpy()\n",
    "test_y = test_data_y\n",
    "\n",
    "val_X = val_data_X.to_numpy()\n",
    "val_y = val_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets setup few functions to visualise the results.\n",
    "\n",
    "(Ignore section if on AWS) It is likely that you won’t have the graphviz package available, in which case you will need to install graphviz. This can be done through the anacoda-navigator interface (environment tab):\n",
    "1. Change the dropbox to “All”\n",
    "2. Search for the packagecpython-graphviz\n",
    "3. Select the python-graphviz package and install (press “apply”)\n",
    "\n",
    "If you cant install graphviz don’t worry - you can still complete the lab. Graphviz is nice to be able to see the trees that are being calculated. However, once the trees become complex, visualising them isn’t practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "\n",
    "def get_tree_2_plot(clf):\n",
    "    dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=dataXExpand.columns,  \n",
    "                      class_names=class_labels,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "    graph = graphviz.Source(dot_data) \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_acc_scores(clf, train_X, train_y, val_X, val_y):\n",
    "    train_pred = clf.predict(train_X)\n",
    "    val_pred = clf.predict(val_X)\n",
    "    \n",
    "    train_acc = f1_score(train_y, train_pred, average='macro')\n",
    "    val_acc = f1_score(val_y, val_pred, average='macro')\n",
    "    \n",
    "    return train_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple decision tree training\n",
    "\n",
    "Lets train a simple decision tree and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree_max_depth = 2   #change this value and observe\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=tree_max_depth, class_weight='balanced')\n",
    "clf = clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtree = get_tree_2_plot(clf)\n",
    "Dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = get_acc_scores(clf,train_X, train_y, val_X, val_y)\n",
    "print(\"Train f1 score: {:.3f}\".format(train_acc))\n",
    "print(\"Validation f1 score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Did we achieve the desired target value? If not what do you thing the above results indicate: over-fitting, under-fitting\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> Based on the answer to the above question, what do you think is the best course of action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> What are the hyper parameters of the `DecisionTreeClassifier`?\n",
    "\n",
    "You may decide to tune the important hyper-paramters of the decision tree classifier (identified in the above question) to get the best performance. As an example I have selected two hyper parameters:  `max_depth` and `min_samples_split`.\n",
    "\n",
    "In this exercise I will be using GridSearch to tune my parameters. Sklearn has a function that do cross validation to tune the hyper parameters called `GridSearchCV`. Lets use this function. \n",
    "\n",
    "*This step may take several steps depending on the performance of your computer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'max_depth':np.arange(2,400, 50), 'min_samples_split':np.arange(2,50,5)}\n",
    "\n",
    "dt_clf = tree.DecisionTreeClassifier(criterion='entropy', class_weight='balanced')\n",
    "Gridclf = GridSearchCV(dt_clf, parameters, scoring='f1_macro')\n",
    "Gridclf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Gridclf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Gridclf.best_score_)\n",
    "print(Gridclf.best_params_)\n",
    "\n",
    "clf = Gridclf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = get_acc_scores(clf,train_X, train_y, val_X, val_y)\n",
    "print(\"Train f1 score: {:.3f}\".format(train_acc))\n",
    "print(\"Validation f1 score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Did we achieve the desired target value? If not what do you thing the above results indicate: over-fitting, under-fitting\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> Based on the answer to the above question, what do you think is the best course of action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post pruning decision trees with cost complexity pruning\n",
    "\n",
    "\n",
    "The DecisionTreeClassifier provides parameters such as min_samples_leaf and max_depth to prevent a tree from overfitting. Those parameters prevent the tree from growing to large size and are examples of pre pruning. \n",
    "\n",
    "Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting. This algorithm finds the node with the ''weakest link'' characterised by an effective alpha. Then the nodes with the smallest effective alpha are pruned first. as the algorithm works after the tree is grown, this is a post pruning technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(class_weight='balanced')\n",
    "path = clf.cost_complexity_pruning_path(train_X, train_y)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following step may take several steps depending on the performance of your computer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha, class_weight='balanced')\n",
    "    clf.fit(train_X, train_y)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [f1_score(train_y, clf.predict(train_X), average='macro') for clf in clfs]\n",
    "val_scores = [f1_score(val_y, clf.predict(val_X), average='macro') for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"f1_score\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, val_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What `ccp_alphas` value would you choose as the best for the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "Lets make many trees using our dataset. If we run the DT algorithm multiple times on same data, it will result in the same tree. To make different trees we can inject some randomness. Select data data points and features to be used in DT algorithm randomly - this process is called creating boor strapped datasets.\n",
    "\n",
    "This is automatically done in sklearn for us in the `RandomForestClassifier`. Lets use that on our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=8, n_estimators=500, class_weight='balanced_subsample', random_state=0)\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = get_acc_scores(clf, train_X, train_y, val_X, val_y)\n",
    "print(\"Train Accuracy score: {:.3f}\".format(train_acc))\n",
    "print(\"Validation Accuracy score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Now tune the hyper parameters of the random forest.</font>**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Is the final model that you get after hyper parameter tuning better than the previous decision tree model? Why?\n",
    "\n",
    "Lets visualise the feature importance of the RF classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feature_importances = clf.feature_importances_\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(dataXExpand.columns[sorted_idx], tree_feature_importances[sorted_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Based on the above figure, do you see any reason to be concerned about the model?\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> If the model uses `duration` to predict the target, what can be an issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Regression Decision Tree\n",
    "\n",
    "A regression decision tree can also be trained.  These are decision trees where the leaf node is a regression function. You will investigate learning regression trees using the boston housing data set from previous labs.\n",
    "\n",
    "The below code snippet will help get you started. Note that it does not make sense to use entropy for generating splits, so the default method from sklearn will be used. Also note that the DecisionTreeRegressor class uses similar pre-pruning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import sklearn \n",
    "\n",
    "# from sklearn import tree\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import metrics\n",
    "# from sklearn import model_selection\n",
    "\n",
    "# Load data\n",
    "\n",
    "# bostonDataTarget = bostonData['MEDV']\n",
    "# bostonDataAttrs = bostonData.drop(columns='MEDV')\n",
    "# trainY, testY, trainX, testX = model_selection.train_test_split(np.array(bostonDataTarget),np.array(bostonDataAttrs), test_size=0.2)\n",
    "# clfBoston = sklearn.tree.DecisionTreeRegressor(max_depth=5, min_samples_split=5)\n",
    "# clfBoston = clfBoston.fit(trainX, trainY)\n",
    "# predictions = clfBoston.predict(testX)\n",
    "# metrics.mean_squared_error(testY, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> How does the error on the regression decision tree compare to the best results you have found in previous labs?\n",
    "    \n",
    "<span style=\"font-size:1.5em;\">�</span> Find a good set of pre-pruning parameters that minimises the mean square error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
