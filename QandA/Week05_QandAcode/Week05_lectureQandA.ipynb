{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> **Example: Week05 Lecture QandA**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo code for week 04 Lecture QandA.\n",
    "\n",
    "**Disclaimer:** The code is done quickly to demonstrate some important concepts in decision trees and, should not be considered as an adequate approach to solve the tasks mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this excersise lets use the `Diagnostic Wisconsin Breast Cancer Database`. The breast cancer dataset is a classic and very easy binary classification dataset.\n",
    " \n",
    "    =================   ==============\n",
    "    Classes                          2\n",
    "    Samples per class    212(M),357(B)\n",
    "    Samples total                  569\n",
    "    Dimensionality                  30\n",
    "    Features            real, positive\n",
    "    =================   ==============\n",
    "    \n",
    "more information on features of the dataset can be obained from: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "dataX = data['data']\n",
    "datay = data['target']\n",
    "feature_names = data['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets separate the data into training and validation.\n",
    "\n",
    "**I am not going to separate test data for this example. Note that this is extremely undesirable for ML. However, it is not useful here as this is just an example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(dataX, datay, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a function to plot the tree from the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "\n",
    "def get_tree_2_plot(clf):\n",
    "    dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=data.feature_names,  \n",
    "                      class_names=data.target_names,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "    graph = graphviz.Source(dot_data) \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another function to get the training and vladation accuracies. I will use balanced accuracy for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def get_acc_scores(clf, train_X, train_y, val_X, val_y):\n",
    "    train_pred = clf.predict(train_X)\n",
    "    val_pred = clf.predict(val_X)\n",
    "    \n",
    "    train_acc = balanced_accuracy_score(train_y, train_pred)\n",
    "    val_acc = balanced_accuracy_score(val_y, val_pred)\n",
    "    \n",
    "    return train_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X,y, xlabel, ylabel, poly=None):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    plt.subplot(1, 1,  1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    if poly != None:\n",
    "        XX = np.c_[xx.ravel(), yy.ravel()]\n",
    "        XX_poly = poly.transform(XX)\n",
    "        Z = clf.predict(XX_poly)\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start lets fit a decition tree to training data. Yo can change the depth of the decition tree and observe the resulting decition tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_max_depth = 2   #change this value and observe\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=tree_max_depth)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtree = get_tree_2_plot(clf)\n",
    "Dtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = get_acc_scores(clf, X_train, y_train, X_val, y_val)\n",
    "print(\"Train Accuracy score: {:.3f}\".format(train_acc))\n",
    "print(\"Validation Accuracy score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if we do not set the max_length parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "Dtree = get_tree_2_plot(clf)\n",
    "Dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a better model than the first tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = get_acc_scores(clf, X_train, y_train, X_val, y_val)\n",
    "print(\"Train Accuracy score: {:.3f}\".format(train_acc))\n",
    "print(\"Validation Accuracy score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the max_depth parameter controlls the complexity of the model produced by the decition tree algorithm. Therefore it is regularized for decition trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tune the max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [2,4,5,8,10,11]\n",
    "\n",
    "train_hold = list()\n",
    "val_hold = list()\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc, val_acc = get_acc_scores(clf, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    train_hold.append(train_acc)\n",
    "    val_hold.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depths, train_hold, 'r--')\n",
    "plt.plot(max_depths, val_hold, 'b-')\n",
    "plt.legend(['Train Acc','Val Acc'])\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_depth` is not the only regularized you can apply to decition trees. there are many other variables that can be used to regularize a DT. Please explore the sklearn documentation [Decision Trees](https://scikit-learn.org/stable/modules/tree.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_splits = [2,4,5,8,10,11]\n",
    "train_hold = list()\n",
    "val_hold = list()\n",
    "\n",
    "for min_samples_split in min_samples_splits:\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=min_samples_split)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc, val_acc = get_acc_scores(clf, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    train_hold.append(train_acc)\n",
    "    val_hold.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depths, train_hold, 'r--')\n",
    "plt.plot(max_depths, val_hold, 'b-')\n",
    "plt.legend(['Train Acc','Val Acc'])\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of the decision boundary & complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_used = (feature_names == 'mean concave points') + (feature_names == 'worst perimeter')\n",
    "feature_used = [i for i, x in enumerate(feature_used) if x]\n",
    "X_train_2d = X_train[:, feature_used]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')  # , max_depth=2\n",
    "clf = clf.fit(X_train_2d, y_train)\n",
    "\n",
    "plot_decision_boundary(clf, X_train_2d,y_train, 'mean concave points', 'worst perimeter', poly=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=['mean concave points', 'worst perimeter'], class_names=data.target_names,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post pruning decision trees with cost complexity pruning\n",
    "\n",
    "\n",
    "The DecisionTreeClassifier provides parameters such as min_samples_leaf and max_depth to prevent a tree from overfiting. Those parameters prevent the tree from growing to large size and are examples of pre prunning. \n",
    "\n",
    "Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting. This algorithm finds the node with the ''weakest link'' characterized by an effective alpha. Then the nodes with the smallest effective alpha are pruned first. as the algorithm works after the tree is grown, this is a post pruning technique. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_val, y_val) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "lets make many trees using our dataset. if we run the DT algorithm multiple times on same data, it will result in the same tree. To make different trees we can inject some randomness. Select data datapoints and features to be used in DT algorithm randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, m = X_train.shape\n",
    "num_trees = 10\n",
    "\n",
    "clfs = list()\n",
    "col_indexs = list()\n",
    "val_acc_hold = list()\n",
    "for i in range(0,num_trees):\n",
    "    data_index = np.random.choice(N, N, replace=True)\n",
    "    col_index = np.random.choice(m, np.floor(m/2).astype(np.int), replace=False)\n",
    "\n",
    "    X_train1 = X_train[data_index, :][:, col_index]\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "    clf = clf.fit(X_train1, y_train[data_index].ravel())\n",
    "    \n",
    "    clfs.append(clf)\n",
    "    col_indexs.append(col_index)\n",
    "\n",
    "    X_val1 = X_val[:, col_index]\n",
    "\n",
    "    train_acc, val_acc = get_acc_scores(clf, X_train1, y_train[data_index].ravel(), X_val1, y_val)\n",
    "    print(\"Validation Accuracy score: {:.3f}\".format(val_acc))\n",
    "    val_acc_hold.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets aggreagate the output of all the trees via voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_hat = np.zeros_like(y_val)\n",
    "for clf, col_index in  zip(clfs, col_indexs):\n",
    "    y_hat = clf.predict(X_val[:, col_index])\n",
    "    y_val_hat = y_val_hat + y_hat\n",
    "\n",
    "y_val_hat = y_val_hat/num_trees > 0.5\n",
    "val_acc = balanced_accuracy_score(y_val, y_val_hat)\n",
    "print(\"Validation Accuracy score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the resulting ensemble has obtained better performance than any of the individual trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(0,num_trees), val_acc_hold)\n",
    "plt.plot([0,num_trees], [val_acc,val_acc], 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = get_acc_scores(clf, X_train, y_train, X_val, y_val)\n",
    "print(\"Train Accuracy score: {:.3f}\".format(train_acc))\n",
    "print(\"Validation Accuracy score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the hyper-parameters of random forest? Need to train the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feature_importances = clf.feature_importances_\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(feature_names[sorted_idx], tree_feature_importances[sorted_idx])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
