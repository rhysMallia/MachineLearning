{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> **Example: Week04 Lecture QandA**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo code for week 04 Lecture QandA.\n",
    "\n",
    "**Disclaimer:** The code is done quickly to demonstrate some important concepts in \"Evaluating a hypotheses\" and, should not be considered as an adequate approach to solve the tasks mentioned.\n",
    "\n",
    "In this example we will look into classifying images of hand writtend digits. An example from the dataset we are working with is given below:  \n",
    "<img src=\"MNIST_6_0.png\">\n",
    "\n",
    "\n",
    "The digits dataset consists of 8x8 pixel images of digits. \n",
    "The images attribute of the dataset stores 8x8 arrays of grayscale values for each image. To apply a classifier on this data, we need to flatten the images, turning each 2-D array of grayscale values from shape (8, 8) into shape (64,).\n",
    "\n",
    "sklearn has already done this for us and a subset of MNIST data is available in sklearn datasets.\n",
    "\n",
    "The task we are interested in is: Given the pixel intensities (64 dimensional attribute vector) predict what digit(0 to 9) the image represent.\n",
    "\n",
    "Lets first read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise some data. As data are images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(15,5))\n",
    "plt.gray() \n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(digits.images[i]) \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(['Class = ', str(digits.target[i])])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put the data to a dataframe. This is optional - we can directly work with the data from the sklearn dataset if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=digits.data, columns=[\"pixel_\"+ str(i+1) for i in range(0,64)] )\n",
    "data['class'] = digits.target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now examine the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['class'] ,alpha=0.3, color='b', density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_0 = data[data['class'] == 0]\n",
    "# data_8 = data[data['class'] == 8]\n",
    "# print(data_0.shape, data_8.shape)\n",
    "\n",
    "# data = data_0.append(data_8, ignore_index=True)\n",
    "# data['class'] = data['class'] == 8\n",
    "# print(data_0.shape, data_8.shape, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some visualisation of the pixel intensities according to the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(20,10))\n",
    "i=1\n",
    "for col in data.columns:\n",
    "  \n",
    "  if col != 'class':\n",
    "    plt.subplot(8,8,i)\n",
    "    sns.boxplot(x='class',y=col,data=data)\n",
    "    i = i+1\n",
    "    plt.title(col)\n",
    "\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you observe?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the performance (evaluation) metric\n",
    "\n",
    "Based on the above observations and the description of the task, what do you think is a **good performance measure for this task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many performance metrics that apply to this problem such as `accuracy_score`, `f1_score`, etc. More information on performance metrics available in sklearn can be found at: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "However as the most **intuitive** out of these `accuracy_score` can be applied to this problem lets stick with it (if the classes are not balanced, then we have to go for another metric). \n",
    "\n",
    "What is the **target value**: In literature we can see that other people have achieved around **99% accuracy** in similar tasks. Therefore lets set out goal at that.\n",
    "\n",
    "\n",
    "Next **what data should we use to evaluate the performance?**\n",
    "\n",
    "\n",
    "We can generate \"simulated\" unseen data in several methods\n",
    "1. Hold-Out validation\n",
    "2. Cross-Validation\n",
    "\n",
    "\n",
    "Lets look at how this is done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out Validation\n",
    "\n",
    "In hold out validation we divide the data into 3 subsets:\n",
    "1. Training: to obtaining the parameters or the weights of the hypothesis\n",
    "2. Validation: for tuning hyper-parameters and model selection.\n",
    "3. To evaluate the performance of the developed model. DO NOT use this split to set or tune ant element of the model.\n",
    "\n",
    "For this example lets divide the data into 60/20/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, shuffle=True,random_state=0)\n",
    "    \n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_, val_data = train_test_split(train_data, test_size=0.25, shuffle=True,random_state=0)\n",
    "    \n",
    "print(train_data.shape[0], val_data.shape[0], test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data_.drop(['class',], axis=1).to_numpy()\n",
    "train_y = train_data_[['class']].to_numpy()\n",
    "\n",
    "test_X = test_data.drop(['class',], axis=1).to_numpy()\n",
    "test_y = test_data[['class']].to_numpy()\n",
    "\n",
    "val_X = val_data.drop(['class',], axis=1).to_numpy()\n",
    "val_y = val_data[['class']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the values for the features are between 0-16 (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html). Lets scale them to 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X/16.0\n",
    "test_X = test_X/16.0\n",
    "val_X = val_X/16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup a function to print train/val accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def print_accuracy_scores(clf, train_X, train_y, val_X, val_y):\n",
    "    train_pred = clf.predict(train_X)\n",
    "    val_pred = clf.predict(val_X)\n",
    "    \n",
    "    train_acc = accuracy_score(train_y, train_pred)\n",
    "    val_acc = accuracy_score(val_y, val_pred)\n",
    "    \n",
    "    print(\"Train Accuracy score: {:.3f}\".format(train_acc))\n",
    "    print(\"Validation Accuracy score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "We need to select a baseline mode to do this task. I am going to select linear logistic regression for this example.\n",
    "\n",
    "*There are better models than this, however we only know of one model we can use for this problem at the moment, therefore out choices are limited and the decision is simple.*\n",
    "\n",
    "If we had other options, we need to use our knowledge on those techniques and  the EDA to select the best base model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='liblinear', max_iter=1000).fit(train_X, train_y.ravel())\n",
    "\n",
    "print_accuracy_scores(clf, train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task the baseline model achieved good performance. However we can see a gap between the Train Accuracy and the Validation Accuracy (generalisation GAP). \n",
    "\n",
    "**What can we do when there is a GAP between Train and Validation performance?**\n",
    "\n",
    "- We can apply regularisation. The process is important. we start with a base model and then improve it based on our observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply regularisation\n",
    "\n",
    "When applying regularisation we need to select the lambda value. For this we can use\n",
    "1. Grid search\n",
    "2. Random search\n",
    "\n",
    "\n",
    "We will do grid search in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_paras = np.logspace(-5, 1, num=100)    # establish the lambda values to test (grid)\n",
    "\n",
    "# Then search\n",
    "train_performace = list()\n",
    "valid_performace = list()\n",
    "\n",
    "for lambda_para in lambda_paras:\n",
    "    clf = LogisticRegression(penalty='l2', C = 1.0/lambda_para, random_state=0, solver='liblinear', max_iter=1000).fit(train_X, train_y.ravel())\n",
    "    \n",
    "    train_pred = clf.predict(train_X)\n",
    "    val_pred = clf.predict(val_X)\n",
    "    \n",
    "    train_acc = accuracy_score(train_y, train_pred)\n",
    "    val_acc = accuracy_score(val_y, val_pred)\n",
    "    \n",
    "    train_performace.append(train_acc)\n",
    "    valid_performace.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the results. When plotting I have made some changes to the values to get an image that looks like the one that is in the lecture slides. This is optional, you can plot the original numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1.0/lambda_para for lambda_para in lambda_paras], [1.0 - tp for tp in train_performace], 'r-')\n",
    "plt.plot([1.0/lambda_para for lambda_para in lambda_paras], [1.0 - vp for vp in valid_performace], 'b--')\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel('Classification Error')\n",
    "plt.xlabel('Model Capacity')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What lambda value would you use? why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', C = 10.0, random_state=0, solver='liblinear', max_iter=1000).fit(train_X, train_y.ravel())\n",
    "print_accuracy_scores(clf, train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there still a GAP between training and validation. Why is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to see what features are important to the classifier we have selected. Instead of using bar graphs as last week. lets present the feature importance as an image. This shows that you need to do output visualisations according to the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.gray() \n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    coef = clf.coef_[i]\n",
    "    coef = coef.reshape((8,8))\n",
    "    plt.imshow(coef) \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(['Class = ', str(digits.target[i])])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for 0 class vs 8\n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.gray() \n",
    "# for i in range(1):\n",
    "#     plt.subplot(2,5,i+1)\n",
    "#     coef = clf.coef_[i]\n",
    "#     coef = coef.reshape((8,8))\n",
    "#     plt.imshow(coef) \n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.title(['Class = ', str(digits.target[i])])\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Performance Metrics\n",
    "\n",
    "We can also use other performance metrics to evaluate out model at this stage\n",
    "\n",
    "classification report is a very useful tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_pred = clf.predict(test_X)\n",
    "    \n",
    "print(classification_report(test_y, test_pred, target_names=[str(i) for i in range(0,10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful tool is the confusion matrix plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "disp = plot_confusion_matrix(clf, test_X, test_y,\n",
    "                                 display_labels=[str(i) for i in range(0,10)],\n",
    "                                 cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "\n",
    "Cross-validation (CV) is the second technique we can use to generate simulated unseen data. Lets see how this is done.\n",
    "\n",
    "Again we need to separate some test data - for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, shuffle=True,random_state=0)\n",
    "    \n",
    "print(train_data.shape[0], test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data_.drop(['class',], axis=1).to_numpy()\n",
    "train_y = train_data_[['class']].to_numpy()\n",
    "\n",
    "test_X = test_data.drop(['class',], axis=1).to_numpy()\n",
    "test_y = test_data[['class']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how CV is done for a given lambda value. The advantage would be that the CV performance metric value would be using the entire training set (not just some portion of it).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "score_hold_val = list()\n",
    "score_hold_train = list()\n",
    "for train_idx, val_idx in kf.split(train_X):\n",
    "    train_x_split = train_X[train_idx,:]\n",
    "    train_y_split = train_y[train_idx,:]\n",
    "    val_x_split = train_X[val_idx,:]\n",
    "    val_y_split = train_y[val_idx,:]\n",
    "    \n",
    "    clf = LogisticRegression(penalty='l2', C = 10, random_state=0, solver='liblinear', max_iter=1000, class_weight='balanced').fit(train_x_split, train_y_split.ravel())\n",
    "    \n",
    "    train_pred = clf.predict(train_x_split)\n",
    "    val_pred = clf.predict(val_x_split)\n",
    "    \n",
    "    train_acc = accuracy_score(train_y_split, train_pred)\n",
    "    val_acc = accuracy_score(val_y_split, val_pred)\n",
    "    \n",
    "    score_hold_train.append(train_acc)\n",
    "    score_hold_val.append(val_acc)\n",
    "    \n",
    "print('Cross validation Error: ' + str(np.mean(score_hold_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to tune the hyper parameter lambda, we need to run the above code for several lambdas and get the best value (Grid search). \n",
    "\n",
    "Running the above is not very convenient. SK-learn has made this easy for us by including a function to do CV. More information on CV with sklearn can be found at: https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "lambda_paras = np.logspace(-5, 4, num=10)\n",
    "\n",
    "cv_results = dict()\n",
    "\n",
    "for lambda_para in lambda_paras:\n",
    "    clf = LogisticRegression(penalty='l1', C = 1.0/lambda_para, solver='liblinear', max_iter=1000)\n",
    "    scores = cross_validate(clf, train_X, train_y.ravel(), scoring=accuracy_scorer, return_estimator=True,return_train_score=True, cv=10)\n",
    "    \n",
    "    cv_results[lambda_para] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "val_means = [1 - np.mean(cv_results[lambda_para]['test_score']) for lambda_para in lambda_paras]\n",
    "val_std = [np.std(cv_results[lambda_para]['test_score']) for lambda_para in lambda_paras]\n",
    "\n",
    "train_means = [1 - np.mean(cv_results[lambda_para]['train_score']) for lambda_para in lambda_paras]\n",
    "train_std = [np.std(cv_results[lambda_para]['train_score']) for lambda_para in lambda_paras]\n",
    "\n",
    "ax.errorbar([1.0/lambda_para for lambda_para in lambda_paras], \n",
    "            val_means,\n",
    "            yerr=val_std)\n",
    "\n",
    "ax.errorbar([1.0/lambda_para for lambda_para in lambda_paras], \n",
    "            train_means,\n",
    "            yerr=train_std)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.xlabel('Model Capacity')\n",
    "plt.legend(['Validation','Training',])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the lambda value, how can we get the model?\n",
    "\n",
    "One way to do this is to pick the classifier that is closest to the mean performance from the best lambda value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = 0.1    # best lambda according to the above figure\n",
    "\n",
    "# get the split that has the closest performance value to the mean performance\n",
    "best_classifier_inx = np.argmin(np.abs(cv_results[0.1]['test_score'] - np.mean(cv_results[0.1]['test_score'])))\n",
    "\n",
    "clf = cv_results[0.1]['estimator'][best_classifier_inx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf.predict(test_X)\n",
    "    \n",
    "print(classification_report(test_y, test_pred, target_names=[str(i) for i in range(0,10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
