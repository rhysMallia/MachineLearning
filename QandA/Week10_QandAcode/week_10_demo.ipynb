{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'>  </font></div>\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> **Example: Week10 Lecture QandA**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Computer Vision\n",
    "\n",
    "First lets have a look at some of the common operations used in traditional computer vision. Namely, Convolutions and feture extractors.\n",
    "\n",
    "Lets first read an image of my Dog: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgba2rgb, rgb2gray\n",
    "\n",
    "image = rgba2rgb(imread('shady.png', as_gray=False, plugin=None))\n",
    "\n",
    "# Plot image\n",
    "plt.imshow(image, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say we want to make the above image smooth (blur). In computer vision we do this through using the convolution operation. To smooth we need to use a gaussian kernal. We need to set the Standard deviation (sigma) for Gaussian kernel which will controll the smoothing. Large sigma will blur the image more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gaussian\n",
    "\n",
    "smooth_image_1 = gaussian(image, sigma=1, multichannel=True)\n",
    "smooth_image_10 = gaussian(image, sigma=10, multichannel=True)\n",
    "\n",
    "# Plot image\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(smooth_image_1)\n",
    "plt.title(\"Gaussian sigma = 1\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(smooth_image_10)\n",
    "plt.title(\"Gaussian sigma = 10\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer vision , edge detection is often used to reduce the amount of information in an image. Lets try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "grayscale = rgb2gray(image)\n",
    "smooth_image_1 = gaussian(grayscale, sigma=2)\n",
    "edge_sobel_h = filters.sobel_h(smooth_image_1)\n",
    "edge_sobel_v = filters.sobel_v(smooth_image_1)\n",
    "\n",
    "# Plot image\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(edge_sobel_h, cmap=plt.cm.coolwarm)\n",
    "plt.axis('off')\n",
    "plt.title(\"Horizontal edges\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(edge_sobel_v, cmap=plt.cm.coolwarm)\n",
    "plt.axis('off')\n",
    "plt.title(\"Vertical edges\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer vision researchers have developed useful feature representations that are useful for many applications. One of them is the HOG features.\n",
    "We will now extract HOG features (Histogram of Gradients). More information on HoG features can be found at [website](https://www.learnopencv.com/histogram-of-oriented-gradients/). There are several other feature types availble with sk-image see [page](https://scikit-image.org/docs/dev/api/skimage.feature.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_features, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "\n",
    "# Rescale histogram for better display\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, .1))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
    "\n",
    "ax1.axis('off')\n",
    "ax1.imshow(image, cmap=plt.cm.gray)\n",
    "ax1.set_title('Input image')\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "ax2.set_title('Histogram of Oriented Gradients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image Shape: \", image.shape, \"Flattened shape : \", np.prod(image.shape))\n",
    "print(\"HoG feature Shape: \", hog_features.shape, \"Flattened shape : \", np.prod(hog_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `hog_features` can be used as the input to ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning (Convolutional Neural Networks)\n",
    "\n",
    "In deep learning we skip this feature construction stage by using a deep network with many layers. The process to develop a deep network will be very similer to how we developed the shallow network. The main difference would be in the architecture selection. Usually instead of selecting a totally new architechture, one would investigate the existing architectures in literature and use it as the base model. Then we can modify it based on the observations we make (e.g. overfitting and underfitting). \n",
    "\n",
    "\n",
    "Lets try to solve the problem from last weeks lecture QandA Example. The task was to predict the face orientation given an image of a human face. \n",
    "\n",
    "Lets start by setting up the data. This is exactly similer to what we did last week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('./faces_TM.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "\n",
    "val_persons = ['sz24', 'megak', 'night', 'choon', 'kawamura']\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "image_list = []\n",
    "for filepath in glob.glob('./faces_TM/*/*.jpg', recursive=True): #assuming gif\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    person = filepath.split(\"/\")[-2]\n",
    "    label = filename.split(\"_\")[1]\n",
    "    val_train = person in val_persons\n",
    "    image_list.append((filepath, label, int(val_train)))\n",
    "    \n",
    "# Create a data frame\n",
    "data = pd.DataFrame(data=image_list, columns=['image_path', 'label', 'isVal'])\n",
    "\n",
    "d = {'left':0, 'right':1, 'straight':2, 'up':3}\n",
    "data['labels_num'] = data['label'].map(d, na_action='ignore')\n",
    "\n",
    "train_df = data[data['isVal']==0].reset_index()\n",
    "validation_df = data[data['isVal']==1].reset_index()\n",
    "print('Train size: {}, Val size: {}'.format(train_df.shape[0], validation_df.shape[0] ) )\n",
    "N_train_images = train_df.shape[0]\n",
    "N_val_images = validation_df.shape[0]\n",
    "\n",
    "train_df.to_csv('TrainData.csv')\n",
    "validation_df.to_csv('ValData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup data loaders as we did last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=validation_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up base model\n",
    "\n",
    "We need to select a base model for this task. What would be a good model. According to the lecture we can select the laNet model. This is a simple model that can operate on images that are small. Lets implement it. We have made few modifications here to the original lanet architechture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lambda = 0.001\n",
    "OUTPUT_CLASSES = 4\n",
    "\n",
    "model_leNet = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28, 3)),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,0], -1, name=None)),\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train this model (same as last week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_leNet.compile(optimizer=sgd,\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model_leNet.fit_generator(train_generator, \n",
    "                                    validation_data = validation_generator, \n",
    "                                    epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], 'r--')\n",
    "plt.plot(history.history['val_loss'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['categorical_accuracy'], 'r--')\n",
    "plt.plot(history.history['val_categorical_accuracy'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "If the model is overfirring we can use all the techniques we discussed last week to companseate for that. In addition a common technique used in deep CNN for regularization is data augmentation. \n",
    "\n",
    "In data augmentation, we randomly perterbate the original dataset to form a larger dataset and use that for training. \n",
    "\n",
    "The keras data grnrrator can do this for us. However we need to pick the data augmentation techniques that are appropriate for the task. Lets see how this is done. We apply data augmentation only for trainig data (not for validation or test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last',\n",
    "                                  rotation_range=15, width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=validation_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_leNet_aug = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28, 3)),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,0], -1, name=None)),\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))\n",
    "])\n",
    "\n",
    "model_leNet_aug .compile(optimizer=sgd,\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model_leNet_aug .fit_generator(train_generator, \n",
    "                                    validation_data = validation_generator, \n",
    "                                    epochs=150, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], 'r--')\n",
    "plt.plot(history.history['val_loss'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['categorical_accuracy'], 'r--')\n",
    "plt.plot(history.history['val_categorical_accuracy'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
