{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'>  </font></div>\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> **Example: Week09 Lecture QandA**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this demo we will use MLP to predict the orientation of a face in a 32 by 32 pixel image. The data set used is from: [Chaper 4 of Machine Learning book by Tom Mitchell](http://www.cs.cmu.edu/~tom/faces.html)\n",
    "\n",
    "The faces of 20 different people are captured at 4 orientations: Left, Right, Up, Straight. Images from each individual is in a separate folder and the label (orientation) for a specific image is given by the image name.  `glickman_up_neutral_sunglasses.pgm -> up`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and setting up dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Face dataset. The dataset can be downloaded from canvas (or bitbucket). You can upload it to the notebook work environment and unzip using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('./faces_TM.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the validation-training data. A good practice would be to hold out some individuals for validation. This will eliminate possible overlaps between train/val splits. Lets hold out the last 5 individuals for validation. **To make things simple, We will not be using a test split for this example. This is not good practice. Yoiu should setup a tes set in your experients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_persons = ['sz24', 'megak', 'night', 'choon', 'kawamura']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the sub folders and read all the image names and create a data frame with relevant information that can be used for training and testing the MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "image_list = []\n",
    "for filepath in glob.glob('./faces_TM/*/*.jpg', recursive=True): #assuming gif\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    person = filepath.split(\"/\")[-2]\n",
    "    label = filename.split(\"_\")[1]\n",
    "    val_train = person in val_persons\n",
    "    image_list.append((filepath, label, int(val_train)))\n",
    "    \n",
    "# Create a data frame\n",
    "data = pd.DataFrame(data=image_list, columns=['image_path', 'label', 'isVal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the lable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is your observation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot some random images and observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_inx = np.random.choice(100, 4)\n",
    "rand_data = data.loc[r_inx,'image_path']\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "for i, image_path in enumerate(rand_data):\n",
    "    im = np.asarray(Image.open(image_path))\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(im,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert string labels to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'left':0, 'right':1, 'straight':2, 'up':3}\n",
    "data['labels_num'] = data['label'].map(d, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate two data frames for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data['isVal']==0].reset_index()\n",
    "validation_df = data[data['isVal']==1].reset_index()\n",
    "print('Train size: {}, Val size: {}'.format(train_df.shape[0], validation_df.shape[0] ) )\n",
    "N_train_images = train_df.shape[0]\n",
    "N_val_images = validation_df.shape[0]\n",
    "\n",
    "train_df.to_csv('TrainData.csv')\n",
    "validation_df.to_csv('ValData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now design and train an extremely simple neural network (MLP) with one hidden layer.\n",
    "Two of the choices are made for us by the data.\n",
    "We have 28x28x3 features and four classes, so the input layer must have 2352 units, and the output layer must have 4 units.\n",
    "We only have to define the hidden layers.\n",
    "We're only going to have one hidden layer for this project, and we'll give it 256 nodes/neurons/units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model - Base model\n",
    "\n",
    "We will use tensorflow to build our neural network and train. To be precise, we will be using Keras which is a high level API which is part of tensorflow.\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model and finally training the model.\n",
    "\n",
    "### Set up the layers\n",
    "The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n",
    "\n",
    "Most neural networks consists of chaining together simple layers. Most layers, such as `tf.keras.layers.Dense`, have parameters that are learned during training.\n",
    "\n",
    "A layer in the MLP is represented by `tf.keras.layers.Dense`. First lets define the dimetions of our neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = (28,28,3)\n",
    "HIDDEN_LAYER_DIM = 256\n",
    "OUTPUT_CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three ways to build a model in tensorflow: \n",
    " - Functional API\n",
    " - Sub-classing\n",
    " - Sequential API\n",
    "\n",
    "We will use the `Sequential` API to build models as it is the simplest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=INPUT_DIM),\n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_DIM, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer in this network, `tf.keras.layers.Flatten`, transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 * 3 = 2352 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n",
    "\n",
    "After the pixels are flattened, the network consists of a sequence of two `tf.keras.layers.Dense` layers. These are densely connected, or fully connected (MLP), neural layers. The first Dense layer has 256 nodes (or neurons). The second (and last) layer returns a logits array with length of 4. Each node contains a score that indicates the current image belongs to one of the 4 classes.\n",
    "\n",
    "We can use `model.summary()` to print the model that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.keras.utils.plot_model` shows the model as a figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "\n",
    "- **Loss function**: This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "- **Optimizer**: This is how the model is updated based on the data it sees and its loss function.\n",
    "- **Metrics**: Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many optimizers that are available but for now, we will use SGD.\n",
    "Here, we have used categorical_crossentropy because there are more than two categories in the output variable.\n",
    "\n",
    "\n",
    "### Data loader\n",
    "As are going to work with much larger and more complicated data set, that neural network are more suited towards, we need to write efficient code to load the data in batches to memory. This is done in keras using Image data generators. \n",
    "\n",
    "To help we will define a loading function that takes the data frames we defined earlier:\n",
    "\n",
    "*It seems that dataloader does not like to read one channel images. It automatically converts them to 3-channel images. Lets ignore this for now.* \n",
    "\n",
    "We can use the `flow_from_dataframe` function in the keras data loader to load a set of images directly from a pandas data frame (the data frame should contain the image names (path) and the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=validation_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, the training the validation data can be loaded.\n",
    "You will need to supply the appropriate directory. Usually each pixels varies from 0-255, but it's highly recommended to normalize them in range of 0-1 to speed up the training process.\n",
    "The dataloader also do a simple normalisation on the pixel values directly.\n",
    "\n",
    "Now we can use a simple model.fit\\_generator() in Keras to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "Next we can train the model. The training is done in the fit function. Since we have already setup a data generator, we can use the fit_generator to do the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator, validation_data = validation_generator, epochs=150, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `history` variable will now hold the information to plot the training curves. Lets plot the infomrmation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], 'r--')\n",
    "plt.plot(history.history['val_loss'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['categorical_accuracy'], 'r--')\n",
    "plt.plot(history.history['val_categorical_accuracy'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you think of the base model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some regularization\n",
    "It seems like the base model is over fitting. Lets now do some incremental updates based on our observations to improve the model. We discussed several techniques to use when a model is overfitting. \n",
    "\n",
    "first, Lets add some ridge penalty. This requires us to create a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lambda = 0.01\n",
    "\n",
    "model_reg = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=INPUT_DIM),\n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_DIM, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the regularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_reg = model_reg.fit_generator(train_generator, validation_data = validation_generator, epochs=150, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_reg.history['loss'], 'r--')\n",
    "plt.plot(history_reg.history['val_loss'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_reg.history['categorical_accuracy'], 'r--')\n",
    "plt.plot(history_reg.history['val_categorical_accuracy'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the model still overfitting? **How can you tune this parameter to balance overfitting/underfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some dropout\n",
    "\n",
    "Another technique to reduce over-fitting in neural networks is Dropout. \n",
    "Lets create a model with dropout and see if that can also help with overfitting (no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lambda = 0.01\n",
    "\n",
    "model_drop = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=INPUT_DIM),\n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_DIM, activation='sigmoid'),\n",
    "    tf.keras.layers.Dropout(.3),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_drop.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_drop = model_drop.fit_generator(train_generator, validation_data = validation_generator, epochs=150, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_drop.history['loss'], 'r--')\n",
    "plt.plot(history_drop.history['val_loss'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_drop.history['categorical_accuracy'], 'r--')\n",
    "plt.plot(history_drop.history['val_categorical_accuracy'], 'b--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the trained model\n",
    "\n",
    "We can also save the trained model (weights) to be used later for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"base_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we can load the model using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.models.load_model('base_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try changing the dropout and check if you can reduce overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on random images from test set\n",
    "\n",
    "Since we dont have an independet test set for this data, we will test on the validation set. **This is clearly not a good stratergy.**\n",
    "\n",
    "First we will create a data generator to get the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size_t = 1\n",
    "\n",
    "# Here the validation is used for testing and this should be changes to a separate test set.\n",
    "test_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=validation_df,\n",
    "        directory='./',\n",
    "        x_col=\"image_path\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(28, 28),\n",
    "        batch_size=batch_size_t,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_inv = {v: k for k, v in d.items()}\n",
    "plt.figure(figsize=(16,4))\n",
    "batches = 0\n",
    "for x,y in test_generator:\n",
    "        batches = batches + 1\n",
    "        y_hat = model_reg.predict(x, verbose=0)\n",
    "        x = np.squeeze(x)\n",
    "        if batches < 5:\n",
    "            plt.subplot(1,5,batches)\n",
    "            plt.imshow(x)\n",
    "            plt.title(\"GT-{}, Pred-{}\".format(d_inv[np.argmax(y[0])], d_inv[np.argmax(y_hat[0])] ))\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
