{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> **Example: Week03 Lecture QandA**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo code for week 03 Lecture QandA.\n",
    "\n",
    "**Disclaimer:** The code is done quickly to demonstrate some important concepts in logistic regression and regularisation and, should not be considered as an adequate approach to solve the tasks mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets use the IRIS dataset to understand logistic regression. This is perhaps the best known database to be found in the pattern recognition literature. The task is to predict the class of iris plant using the features of its flowers.   \n",
    "<img src=\"Large53.jpeg\">\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. class: 0 - Iris Setosa, 1 - Iris Versicolour, 2 - Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "data = pd.DataFrame(iris.data,columns= iris.feature_names)\n",
    "data['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am not going to separate test data for this example. Note that this is extremely undesirable for ML. However, it is not useful here as this is just an example**\n",
    "\n",
    "Some data visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(data['petal length (cm)'].to_numpy(), data['petal width (cm)'], c=data['target'])\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('petal width')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(data['sepal length (cm)'].to_numpy(), data['petal width (cm)'], c=data['target'])\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('petal width')\n",
    "\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.scatter(data['petal length (cm)'].to_numpy(), data['sepal length (cm)'], c=data['target'])\n",
    "# plt.xlabel('petal length (cm)')\n",
    "# plt.ylabel('sepal length (cm)')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate logistic regression with attribute `petal length (cm)`\n",
    "\n",
    "First lets do binary classification where the task is to **predict if a flower is from Iris Virginica or not using petal length (cm)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['target'] == 2\n",
    "X = data['petal length (cm)'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, c=y)\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('IRIS Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,7,100)\n",
    "y_prob = clf.predict_proba(x.reshape(-1, 1))\n",
    "y_class = clf.predict(x.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(X, y, c=y)\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('IRIS Type')\n",
    "\n",
    "plt.plot(x, y_prob[:,1], 'r--')\n",
    "# plt.plot(x, y_class, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate logistic regression with attribute `sepal length (cm)` and `petal width (cm)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['target'] == 2\n",
    "X = data[['sepal length (cm)','petal width (cm)']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.xlim(X[:,0].min(), X[:,0].max())\n",
    "plt.ylim(X[:,1].min(), X[:,1].max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='liblinear', max_iter=1000).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mesh to plot in\n",
    "def plot_decision_boundary(clf, poly=None):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    plt.subplot(1, 1,  1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    if poly != None:\n",
    "        XX = np.c_[xx.ravel(), yy.ravel()]\n",
    "        XX_poly = poly.transform(XX)\n",
    "        Z = clf.predict(XX_poly)\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('petal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(4)\n",
    "poly.fit(X)\n",
    "X_poly = poly.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_poly = LogisticRegression(random_state=0, solver='liblinear', max_iter=1000).fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(clf_poly, poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_poly = LogisticRegression(penalty='l2', C = 1, random_state=0, solver='liblinear', max_iter=1000).fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(clf_poly, poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets now use all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['target'] == 2\n",
    "X = data[['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some data scaling before applying the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(25,30))\n",
    "# for i, col in enumerate(['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']):\n",
    "#     plt.subplot(6,5,i+1)\n",
    "#     plt.hist(X[:,i], alpha=0.3, color='b', density=True)\n",
    "#     plt.title(col)\n",
    "#     plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,30))\n",
    "for i, col in enumerate(['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']):\n",
    "    plt.subplot(6,5,i+1)\n",
    "    plt.hist(X_scaled[:,i], alpha=0.3, color='b', density=True)\n",
    "    plt.title(col)\n",
    "    plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_all = LogisticRegression(random_state=0, solver='liblinear', max_iter=1000).fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)'], clf_all.coef_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 vs L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_all = LogisticRegression(penalty='l2', C = .01, random_state=0, solver='liblinear', max_iter=1000).fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)'], clf_all.coef_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_all = LogisticRegression(penalty='l1', C = 0.1, random_state=0, solver='liblinear', max_iter=1000).fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)'], clf_all.coef_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diabetes\n",
    "\n",
    "In this section we will build a machine learning model to predict whether or not the patients in the dataset have diabetes or not.\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "Data originally from: *Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('diabetes.csv', delimiter=',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some visualisations.\n",
    "\n",
    "**The data visualisations should be carefully selected to represent what information is needed for the modelling task. The visualisations should not be randomly selected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,30))\n",
    "for i, col in enumerate(data.columns):\n",
    "    plt.subplot(6,5,i+1)\n",
    "    plt.hist(data[col], alpha=0.3, color='b', density=True)\n",
    "    plt.title(col)\n",
    "    plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What important information is given in figure for `Outcome`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(20,10))\n",
    "i=1\n",
    "for col in data.columns:\n",
    "  \n",
    "  if col != 'Outcome':\n",
    "    plt.subplot(2,4,i)\n",
    "    sns.boxplot(x='Outcome',y=col,data=data)\n",
    "    i = i+1\n",
    "    plt.title(col)\n",
    "\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "corr = data.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What important information is given in this figure?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting & scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets divide the data to training/validation/test sets\n",
    "\n",
    "**We will learn about splitting the data and the purpose of each split in week 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, shuffle=True,random_state=0)\n",
    "    \n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.25, shuffle=True,random_state=0)\n",
    "    \n",
    "print(train_data.shape[0], val_data.shape[0], test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data.drop(['Outcome',], axis=1).to_numpy()\n",
    "train_y = train_data[['Outcome']].to_numpy()\n",
    "\n",
    "test_X = test_data.drop(['Outcome',], axis=1).to_numpy()\n",
    "test_y = test_data[['Outcome']].to_numpy()\n",
    "\n",
    "val_X = val_data.drop(['Outcome',], axis=1).to_numpy()\n",
    "val_y = val_data[['Outcome']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler().fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n",
    "val_X = scaler.transform(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def print_f1_scores(clf, train_X, train_y, val_X, val_y):\n",
    "    train_pred = clf.predict(train_X)\n",
    "    val_pred = clf.predict(val_X)\n",
    "    \n",
    "    train_f1 = balanced_accuracy_score(train_y, train_pred)\n",
    "    val_f1 = balanced_accuracy_score(val_y, val_pred)\n",
    "    \n",
    "    print(\"Train Accuracy score: {:.3f}\".format(train_f1))\n",
    "    print(\"Validation Accuracy score: {:.3f}\".format(val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with no regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='liblinear', max_iter=1000, class_weight='balanced').fit(train_X, train_y.ravel())\n",
    "\n",
    "print_f1_scores(clf, train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'], clf.coef_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with l1 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_l1 = LogisticRegression(penalty='l1', C = .75, random_state=0, solver='liblinear', max_iter=1000, class_weight='balanced').fit(train_X, train_y.ravel())\n",
    "\n",
    "print_f1_scores(clf_l1, train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'], clf_l1.coef_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next week we will discuss how to select C**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
