{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> **Example: Week07 Lecture QandA**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car with RL\n",
    "Mountain Car is a classic control Reinforcement Learning problem that was first introduced by A. Moore in 1991 [1]. \n",
    "A car is on a one-dimensional track, positioned between two “mountains”. The goal is to drive up the mountain on the right; however, the car’s engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "It can be tricky to find this optimal solution due to the sparsity of the reward. Complex exploration strategies can be used to incentivise exploration of the mountain. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mountain Car Problem definition:\n",
    ">Objective: Get the car to the top of the right hand side mountain.\n",
    "\n",
    ">State: Car's horizontal position and velocity (can be negative).\n",
    "\n",
    ">Action: Direction of push (left, nothing or right).\n",
    "\n",
    ">Reward: -1 for every environment step until success, which incentivises quick solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym \n",
    "OpenAI Gym is a Python package comprising a selection of RL environments, ranging from simple “toy” environments to more challenging environments, including simulated robotics environments and Atari video game environments.\n",
    "It was developed with the aim of becoming a standardized environment and benchmark for RL research.\n",
    "In this Lab, we will use the OpenAI Gym Mountain Car environment to demonstrate how to get started in using this exciting tool and show how Q-learning can be used to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "Lets first import the libraries required for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "!pip install gym\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only uncomment the following block if the visualization of the environment gives an error**. On mac you need to install pyglet version 1.5.11 to get the gym environment to render. The installation will give an error, but it will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyglet==1.5.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with this environment, import and initialize it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `env.reset()` command resets the environemnt and return the initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore the state space and the action space og the MountainCar environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('State space: ', env.observation_space)\n",
    "print('Action space: ', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the state space is a 2-dimensional space, so each state observation is a vector of 2 (float) values, and that the action space comprises three discrete actions (left, nothing or right). By default, the three actions are represented by the integers 0, 1 and 2. How about the state space? What are the limis of the state space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('State space Low: ', env.observation_space.low)\n",
    "print('State space High: ', env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the first state variable (horizontal position) has a range [-1.2, 0.6] and the second state variable (speed) has a range [-0.07, 0.07]. The state space of the environment is a continuous state space, which means that there are infinitely many state-action pairs, making it impossible to build a Q table. As a solution to this problem we can descritize the state space. One simple discritization is to conver the stat espace to a grid with spacing of 0.1 along first element and 0.01 along second element in the state space. The states can be given integer indexes multiply the first element by 10 and the second by 100. lets see the size of discretized state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1\n",
    "print(num_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write a function that will convert a continuous state vector to a descrete one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize state\n",
    "def discretize_state(state, env_low):\n",
    "    state_adj = (state - env_low)*np.array([10, 100])\n",
    "    state_adj = np.round(state_adj, 0).astype(int)\n",
    "    return state_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now make some random actions in the environment and see what the output will be. For this we need a function to plot the output of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "step_index = 0\n",
    "while done != True:\n",
    "    action = env.action_space.sample()    # get a random action from the set of actions\n",
    "    state, reward, done, info = env.step(action) # perform the action and receive new state and reward\n",
    "    d_state = discretize_state(state, env.observation_space.low)\n",
    "    show_state(env, step=step_index, info='State ({},{}) Reward: {}'.format(d_state[0], d_state[1], reward))\n",
    "    step_index = step_index + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the car reach the goal state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q-learning function\n",
    "def QLearning(env, Q, learning, discount, epsilon, episodes):\n",
    "    # Env: The OpenAI gym environment\n",
    "    # Q: Initial Q table\n",
    "    # learning: Learning Rate of Q learing\n",
    "    # discount: discount facotr (gamma)\n",
    "    # epsilon: epsilon for exploration vs exploitation\n",
    "    # episodes: number of episodes to run when learing the Q table\n",
    "    \n",
    "    # Initialize variables to hold rewards\n",
    "    reward_list = []\n",
    "    \n",
    "    # Calculate reduction in epsilon per episode\n",
    "    epsilon_d = (epsilon)/episodes\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        state_adj = discretize_state(state, env.observation_space.low)\n",
    "    \n",
    "        while done != True:   \n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy for explore vs exploitation\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action) \n",
    "            \n",
    "            state2_adj = discretize_state(state2, env.observation_space.low)\n",
    "            \n",
    "            #Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                Q[state_adj[0], state_adj[1],action] = (1-learning)*Q[state_adj[0], state_adj[1],action] + learning*(reward + \n",
    "                                 discount*np.max(Q[state2_adj[0], state2_adj[1]]))\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Update epsilon\n",
    "        if epsilon > 0:\n",
    "            epsilon -= epsilon_d\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q table randomly\n",
    "Q = np.random.uniform(low = -1, high = 1, size = (num_states[0], num_states[1], env.action_space.n))\n",
    "# Run Q-learning algorithm\n",
    "Q = QLearning(env, Q, 0.2, 0.9, 0.8, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how the we can perform the task with the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state_adj = discretize_state(state, env.observation_space.low)\n",
    "done = False\n",
    "step_index = 0\n",
    "while done != True:\n",
    "    action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "    state, reward, done, info = env.step(action)\n",
    "    state_adj = discretize_state(state, env.observation_space.low)\n",
    "    show_state(env, step=step_index, info='State ({},{}) Reward: {}'.format(d_state[0], d_state[1], reward))\n",
    "    step_index = step_index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
